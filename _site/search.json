[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matt Pickard",
    "section": "",
    "text": "regex to Preserve Currency Tokens\n\n\n\n\n\n\n\nregex\n\n\ndata preparation\n\n\n\n\nDevelopment of a regex to preserve currency values in the U.S. Tax Code.\n\n\n\n\n\n\nMar 14, 2022\n\n\nMatt Pickard\n\n\n\n\n\n\n\n\nUsing xgboost and sklearn to Predict Loan Default\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2022\n\n\nMatt Pickard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/using-xgboost-and-sklearn-to-predict-loan-default/index.en.html",
    "href": "posts/using-xgboost-and-sklearn-to-predict-loan-default/index.en.html",
    "title": "Using xgboost and sklearn to Predict Loan Default",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd \n\nIntroduction\nI wanted to combine xgboost with sklearn pipelines to process a pandas DataFrame. Special thanks to Kaggle user M Yasser H for supplying the Loan Default Dataset.\n\n\nLoad the data\n# Load the data\nloan_df = pd.read_csv(\"../data/Loan_Default.csv\")\n\n\nCreate labels and features\nThe loan status (whether or not the customer defaulted on the loan) is the target variable. We’ll extract that out as our labels. The other columns (minus the ID) will serve as our features. And we’ll take a peek at our features.\n# Split out labels and features, encode labels as integers\ny = loan_df['Status']\n\nX = loan_df.loc[:,~loan_df.columns.isin(['ID','Status'])]\n\n\nSimple data exploration\nTake a peek as our feature variables.\nX.head()\n##    year loan_limit             Gender  ... Region Security_Type dtir1\n## 0  2019         cf  Sex Not Available  ...  south        direct  45.0\n## 1  2019         cf               Male  ...  North        direct   NaN\n## 2  2019         cf               Male  ...  south        direct  46.0\n## 3  2019         cf               Male  ...  North        direct  42.0\n## 4  2019         cf              Joint  ...  North        direct  39.0\n## \n## [5 rows x 32 columns]\nNotice there is a mix of categorical and continuous variables.\nLet’s check if there are any missing values.\nX.isnull().sum()\n## year                             0\n## loan_limit                    3344\n## Gender                           0\n## approv_in_adv                  908\n## loan_type                        0\n## loan_purpose                   134\n## Credit_Worthiness                0\n## open_credit                      0\n## business_or_commercial           0\n## loan_amount                      0\n## rate_of_interest             36439\n## Interest_rate_spread         36639\n## Upfront_charges              39642\n## term                            41\n## Neg_ammortization              121\n## interest_only                    0\n## lump_sum_payment                 0\n## property_value               15098\n## construction_type                0\n## occupancy_type                   0\n## Secured_by                       0\n## total_units                      0\n## income                        9150\n## credit_type                      0\n## Credit_Score                     0\n## co-applicant_credit_type         0\n## age                            200\n## submission_of_application      200\n## LTV                          15098\n## Region                           0\n## Security_Type                    0\n## dtir1                        24121\n## dtype: int64\n\n\nPreprocessing\nSince we have a mix of continuous and categorical variables, we’ll setup an imputers for each type of variable. So, we are going to separate teh continous and the categorical varabiles into separate DataFrames.\nFor the continuous variables, we’ll impute the median.\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\n\n# extract numeric columns\nnumeric_mask = (X.dtypes != object)\nnumeric_columns = X.columns[numeric_mask].tolist()\nnumeric_df = X[numeric_columns]\n\n# create \"imputer\", just going to fill missing values with \"missing\"\nnumeric_imputor = DataFrameMapper(\n  [([numeric_feature], SimpleImputer(strategy='median')) for numeric_feature in numeric_df],\n  input_df=True,\n  df_out=True\n  )\nFor the categorical variables, we’ll impute the value ‘missing’.\n# extract categorical features\ncategorical_mask = (X.dtypes == object)\ncategorical_columns = X.columns[categorical_mask].tolist()\ncategorical_df = X[categorical_columns]\n\ncategorical_imputor = DataFrameMapper(\n  [([categorical_feature], SimpleImputer(strategy='constant', fill_value = \"missing\")) for categorical_feature in categorical_df],\n  input_df=True,\n  df_out=True\n  )\n\n\nBuild the pipeline\nWe are going to use sklearn’s DictVectorizer, which operates on numpy arrays/matrices. So to make it compatiable with DataFrames, we’ll create a simple utility class to allow a DataFrame to be passed through the pipeline. Thanks to Chanseok for the dictifier code.\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define Dictifier class to turn df into dictionary as part of pipeline\nclass Dictifier(BaseEstimator, TransformerMixin):\n  def fit(self, X, y=None):\n    return self\n  \n  def transform(self, X):\n    if type(X) == pd.core.frame.DataFrame:\n      return X.to_dict(\"records\")\n    else:\n      return pd.DataFrame(X).to_dict(\"records\")\nNow we build the pipeline. Notice how we use the FeatureUnion to bring the continous and categorical features back together again at the start of the pipeline.\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nimport xgboost as xgb\n\nimputed_df = FeatureUnion([\n  ('num_imputer', numeric_imputor),\n  ('cat_imputer', categorical_imputor)    \n  ])\n  \nxgb_pipeline = Pipeline([\n  (\"featureunion\", imputed_df),\n  ('dictifier', Dictifier()),\n  ('dict_vectorizer', DictVectorizer(sort=False)),\n  (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n  ])\n\n\n3-Fold Cross Validation\nWe’ll use 3-fold cross validation (instead of 10, or something greater) to minimize compute time.\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb_pipeline, X, y, scoring=\"f1\", cv=3)\navg_f1 = np.mean(np.sqrt(np.abs(scores)))\nprint(\"Avg F1 Score:\", avg_f1)\n## Avg F1 Score: 0.9999863537583441\n\n\nConclusions\nThe average F1 score is suspiciously high, so let’s not put much clout in the quality of the model. But it serves the purpose of demonstrating how to pass a ‘pandas’ DataFrame through an sklearn pipeline, preprocess mixed variable (continuous and categorical) data, and build an xgboost classifier.\nJust because it’s bugging me, here are a few things that may need to be improved in this model:\n\nThere is probably a high correlation between the target variable and some feature variables. We can check this quickly. Fille the NAs with zero and correlate it with the loan status (which is 0 and 1).\n\nnumeric_fillna_df = numeric_df.fillna(0)\nnumeric_fillna_df.corrwith(y)\n## year                         NaN\n## loan_amount            -0.036825\n## rate_of_interest       -0.958875\n## Interest_rate_spread   -0.392977\n## Upfront_charges        -0.431183\n## term                   -0.000675\n## property_value         -0.273267\n## income                 -0.044620\n## Credit_Score            0.004004\n## LTV                    -0.267700\n## dtir1                  -0.325613\n## dtype: float64\nWe can see that rate_of_interest has a high inverse correlation with the target variable. However, I did try removing rate_of_interest and still ended up with an F1 score of 0.9999.\n\nFind a better way to impute the missing categorical. Chirag Goyal enumerates some options in this post. I suspect that building a model to predict missing values would be an option. Another simpler option would be to just randomly insert existing values. But, currently, with the imputer in this post, it is essentially treating ‘missing’ as a legit value."
  },
  {
    "objectID": "posts/using-xgboost-and-sklearn-to-predict-loan-default/index.html",
    "href": "posts/using-xgboost-and-sklearn-to-predict-loan-default/index.html",
    "title": "Using xgboost and sklearn to Predict Loan Default",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd \n\n\nIntroduction\nI wanted to combine xgboost with sklearn pipelines to process a pandas DataFrame. Special thanks to Kaggle user M Yasser H for supplying the Loan Default Dataset.\n\n\nLoad the data\n\n# Load the data\nloan_df = pd.read_csv(\"Loan_Default.csv\")\n\n\n\nCreate labels and features\nThe loan status (whether or not the customer defaulted on the loan) is the target variable. We’ll extract that out as our labels. The other columns (minus the ID) will serve as our features. And we’ll take a peek at our features.\n\n# Split out labels and features, encode labels as integers\ny = loan_df['Status']\n\nX = loan_df.loc[:,~loan_df.columns.isin(['ID','Status'])]\n\n\n\nSimple data exploration\nTake a peek as our feature variables.\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      year\n      loan_limit\n      Gender\n      approv_in_adv\n      loan_type\n      loan_purpose\n      Credit_Worthiness\n      open_credit\n      business_or_commercial\n      loan_amount\n      ...\n      income\n      credit_type\n      Credit_Score\n      co-applicant_credit_type\n      age\n      submission_of_application\n      LTV\n      Region\n      Security_Type\n      dtir1\n    \n  \n  \n    \n      0\n      2019\n      cf\n      Sex Not Available\n      nopre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      116500\n      ...\n      1740.0\n      EXP\n      758\n      CIB\n      25-34\n      to_inst\n      98.728814\n      south\n      direct\n      45.0\n    \n    \n      1\n      2019\n      cf\n      Male\n      nopre\n      type2\n      p1\n      l1\n      nopc\n      b/c\n      206500\n      ...\n      4980.0\n      EQUI\n      552\n      EXP\n      55-64\n      to_inst\n      NaN\n      North\n      direct\n      NaN\n    \n    \n      2\n      2019\n      cf\n      Male\n      pre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      406500\n      ...\n      9480.0\n      EXP\n      834\n      CIB\n      35-44\n      to_inst\n      80.019685\n      south\n      direct\n      46.0\n    \n    \n      3\n      2019\n      cf\n      Male\n      nopre\n      type1\n      p4\n      l1\n      nopc\n      nob/c\n      456500\n      ...\n      11880.0\n      EXP\n      587\n      CIB\n      45-54\n      not_inst\n      69.376900\n      North\n      direct\n      42.0\n    \n    \n      4\n      2019\n      cf\n      Joint\n      pre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      696500\n      ...\n      10440.0\n      CRIF\n      602\n      EXP\n      25-34\n      not_inst\n      91.886544\n      North\n      direct\n      39.0\n    \n  \n\n5 rows × 32 columns\n\n\n\nNotice there is a mix of categorical and continuous variables.\nLet’s check if there are any missing values.\n\nX.isnull().sum()\n\nyear                             0\nloan_limit                    3344\nGender                           0\napprov_in_adv                  908\nloan_type                        0\nloan_purpose                   134\nCredit_Worthiness                0\nopen_credit                      0\nbusiness_or_commercial           0\nloan_amount                      0\nrate_of_interest             36439\nInterest_rate_spread         36639\nUpfront_charges              39642\nterm                            41\nNeg_ammortization              121\ninterest_only                    0\nlump_sum_payment                 0\nproperty_value               15098\nconstruction_type                0\noccupancy_type                   0\nSecured_by                       0\ntotal_units                      0\nincome                        9150\ncredit_type                      0\nCredit_Score                     0\nco-applicant_credit_type         0\nage                            200\nsubmission_of_application      200\nLTV                          15098\nRegion                           0\nSecurity_Type                    0\ndtir1                        24121\ndtype: int64\n\n\n\n\nPreprocessing\nSince we have a mix of continuous and categorical variables, we’ll setup an imputers for each type of variable. So, we are going to separate teh continous and the categorical varabiles into separate DataFrames.\nFor the continuous variables, we’ll impute the median.\n\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\n\n# extract numeric columns\nnumeric_mask = (X.dtypes != object)\nnumeric_columns = X.columns[numeric_mask].tolist()\nnumeric_df = X[numeric_columns]\n\n# create \"imputer\", just going to fill missing values with \"missing\"\nnumeric_imputor = DataFrameMapper(\n  [([numeric_feature], SimpleImputer(strategy='median')) for numeric_feature in numeric_df],\n  input_df=True,\n  df_out=True\n  )\n\nFor the categorical variables, we’ll impute the value ‘missing’.\n\n# extract categorical features\ncategorical_mask = (X.dtypes == object)\ncategorical_columns = X.columns[categorical_mask].tolist()\ncategorical_df = X[categorical_columns]\n\ncategorical_imputor = DataFrameMapper(\n  [([categorical_feature], SimpleImputer(strategy='constant', fill_value = \"missing\")) for categorical_feature in categorical_df],\n  input_df=True,\n  df_out=True\n  )\n\n\n\nBuild the pipeline\nWe are going to use sklearn’s DictVectorizer, which operates on numpy arrays/matrices. So to make it compatiable with DataFrames, we’ll create a simple utility class to allow a DataFrame to be passed through the pipeline. Thanks to Chanseok for the dictifier code.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define Dictifier class to turn df into dictionary as part of pipeline\nclass Dictifier(BaseEstimator, TransformerMixin):\n  def fit(self, X, y=None):\n    return self\n  \n  def transform(self, X):\n    if type(X) == pd.core.frame.DataFrame:\n      return X.to_dict(\"records\")\n    else:\n      return pd.DataFrame(X).to_dict(\"records\")\n\nNow we build the pipeline. Notice how we use the FeatureUnion to bring the continous and categorical features back together again at the start of the pipeline.\n\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nimport xgboost as xgb\n\nimputed_df = FeatureUnion([\n  ('num_imputer', numeric_imputor),\n  ('cat_imputer', categorical_imputor)    \n  ])\n  \nxgb_pipeline = Pipeline([\n  (\"featureunion\", imputed_df),\n  ('dictifier', Dictifier()),\n  ('dict_vectorizer', DictVectorizer(sort=False)),\n  (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n  ])\n\nC:\\Users\\A1849763\\Anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning:\n\npandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n\n\n\n\n\n3-Fold Cross Validation\nWe’ll use 3-fold cross validation (instead of 10, or something greater) to minimize compute time.\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb_pipeline, X, y, scoring=\"f1\", cv=3)\navg_f1 = np.mean(np.sqrt(np.abs(scores)))\nprint(\"Avg F1 Score:\", avg_f1)\n\nAvg F1 Score: 0.9999863537583441\n\n\n\n\nConclusions\nThe average F1 score is suspiciously high, so let’s not put much clout in the quality of the model. But it serves the purpose of demonstrating how to pass a ‘pandas’ DataFrame through an sklearn pipeline, preprocess mixed variable (continuous and categorical) data, and build an xgboost classifier.\nJust because it’s bugging me, here are a few things that may need to be improved in this model:\n\nThere is probably a high correlation between the target variable and some feature variables. We can check this quickly. Fille the NAs with zero and correlate it with the loan status (which is 0 and 1).\n\n\nnumeric_fillna_df = numeric_df.fillna(0)\nnumeric_fillna_df.corrwith(y)\n\nyear                         NaN\nloan_amount            -0.036825\nrate_of_interest       -0.958875\nInterest_rate_spread   -0.392977\nUpfront_charges        -0.431183\nterm                   -0.000675\nproperty_value         -0.273267\nincome                 -0.044620\nCredit_Score            0.004004\nLTV                    -0.267700\ndtir1                  -0.325613\ndtype: float64\n\n\nWe can see that rate_of_interest has a high inverse correlation with the target variable. However, I did try removing rate_of_interest and still ended up with an F1 score of 0.9999.\n\nFind a better way to impute the missing categorical. Chirag Goyal enumerates some options in this post. I suspect that building a model to predict missing values would be an option. Another simpler option would be to just randomly insert existing values. But, currently, with the imputer in this post, it is essentially treating ‘missing’ as a legit value."
  },
  {
    "objectID": "posts/regex-preserve_currency_tokens/index.html",
    "href": "posts/regex-preserve_currency_tokens/index.html",
    "title": "regex to Preserve Currency Tokens",
    "section": "",
    "text": "Introduction\nI’ve been working to collect a corpus of tax legislation, regulations, and court cases. At the core of my corpus is the United States Code Title 26 (USC26), otherwise known as the Internal Revenue Code or the U.S. Tax Code. I’m using the corpus to train a tax-specific word embedding. Word embeddings capture similarity semantics of tokens (words, or multi-word phrases) by encoding them into a n-dimensional space. Similar tokens are embedded into the n-dimensional space in close proximity.\nSince currency values may have important semantic meaning in the USC–for instance, tax bracket boundaries–I want to preserve them so the tokenizer does not break them apart. To do this, I want to replace spaces, commas, decimals, and dollar signs with underscores. Here are some examples:\n\n\n\nOriginal\nPreserved\n\n\n\n\n$172,175\n172_175_usd\n\n\n$2.5 million\n2_5_million_usd\n\n\n$56,750.25\n56_750_25_usd\n\n\n\n\n\nImplementation\nThe approach I took was to first match the different currency formats and then pass the match to a helper function that would replace spaces, commas, decimals, and dollar signs with underscores.\nHere is the pattern I created to match currency tokens:\n\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n\n\n\\$ matches a dollar sign.\n\\d{0,3} The number after the dollar sign and before a comma or decimal boundary. Looking for 0 to 3 digits allows for these patterns: $.75, $125,000, etc.\n(\\,\\d{3}){0,4}(\\.\\d{1,2})? matches repeated comma boundaries (if they exist) and two-decimal places for cents (if they exist). More specifically:\n\n(\\,\\d{3}) matches “,xxx”. With the addition of {0,4} it matches “,xxx” repeatedly–up to trillions (which is probably overkill for the USC26 because most large currency values have “million” or “billion” text suffixes).\n(\\.\\d{1,2})? matches an optional cents.\n\n( (million|billion))? just checks for a text suffix (the ? at the end makes it optional.)\n\nNext, here is a simple helper function to replace spaces, commas, decimals, and dollar signs with underscores and tack on “_usd” at the end. Python’s string replace() method is faster than the regex sub(), so I went with replace().\n\nimport re\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n\nNow I need a test string.\n\ntest_string = \"I made $755.34 billion this year, $5.34 million last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45 in case that's a more realistic salary in my life.\"\n\nThen I compile the regex and replace matches. Notice that the second argument of sub() can take a method that returns a string. I leverage this to call the helper function.\n\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n\n\"I made 755_34_billion_usd this year, 5_34_million_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd in case that's a more realistic salary in my life.\"\n\n\nHere is the full Python implementation.\n\nimport time\nimport timeit\n\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n\ntest_string = \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n\n\n\nR Implementation\nI was curious if python was that much faster than R. So, here is an R implementation. Like Python’s sub(), the str_replace_all function can take a helper function.\n\nlibrary(stringr)\n\npattern <- '\\\\$\\\\d{0,3}(\\\\,\\\\d{3}){0,4}(\\\\.\\\\d{1,2})?( (million|billion))?'\n\nreplace_currency_parts <- function(match) {\n  \n  text <- match %>% \n    str_replace_all(\" \", \"_\") %>% \n    str_replace_all(\"\\\\.\", \"_\") %>% \n    str_replace_all(\"\\\\,\", \"_\")\n    \n  if (str_sub(text, 1, 1) == \"$\"){\n    text <- str_c(str_sub(text, 2), \"_usd\")\n  }\n    \n  return(text)\n}\n\ntest_string <-  \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\nstr_replace_all(test_string, pattern, replace_currency_parts)\n\n[1] \"I made 355_34_million_usd this year, 435_34_billion_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd.\"\n\n\n\n\nWhich is faster?\nSo, which is faster?\nI compare the Python and R implementations by taking the average of 1000 executions of the code. Since the {stringr} package in R is higher level–there’s no option to compile the regex ahead of time and keep it out of the loop–I placed the Python regex compile code inside the loop to make a more fair comparison. In both cases, the units are milliseconds. NOTE: The python time() method returns seconds, so I convert.\n\nfrom time import time\n\nstart_ms = int(round(time() * 1000))\nfor i in range(1000):\n  compiled_pattern = re.compile(pattern)\n  str = compiled_pattern.sub(replace_currency_parts, test_string)\nend_ms = int(round(time() * 1000))\n\nprint(\"elapsed ms = \", (end_ms-start_ms)/1000)\n\nelapsed ms =  0.021\n\n\n\nlibrary(microbenchmark)\n\nmicrobenchmark::microbenchmark(str_replace_all(test_string, pattern, replace_currency_parts), times = 1000)\n\nUnit: milliseconds\n                                                          expr    min      lq\n str_replace_all(test_string, pattern, replace_currency_parts) 1.9412 2.00845\n     mean median     uq    max neval\n 2.181681 2.0652 2.2058 9.9199  1000\n\n\nI don’t know what overhead is included in the {stringr} package, but it’s about two orders of magnitude slower than Python. I suspect it is because the Python replace() string method is quite speedy."
  }
]