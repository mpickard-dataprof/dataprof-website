[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "regex to Preserve Currency Tokens\n\n\n\n\n\n\n\nregex\n\n\ndata preparation\n\n\n\n\nDevelopment of a regex to preserve currency values in the U.S. Tax Code.\n\n\n\n\n\n\nMar 14, 2022\n\n\nMatt Pickard\n\n\n\n\n\n\n\n\nUsing xgboost and sklearn to Predict Loan Default\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2022\n\n\nMatt Pickard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Education\nBrigham Young University | Provo, UT\nB.S. in Computer Engineering | 2002\nBrigham Young University | Provo, UT\nMBA in Operations and Information Systems | 2008\nUniversity of Arizona | Tucson, AZ\nPh.D. in Management Information Systems | 2012\n\n\nExperience\nNational Instruments\nApplications Engineer | 2002 - 2004\nSoftware Engineer / Project Manager | 2004 - 2006\nRaytheon Missile Systems\nTechnical Instructor | 2010\nSandia National Labs\nFaculty Summer Intern - Data Analytics | 2017\nUniversity of New Mexico\nAssistant Professor of AIS and Analytics | 2012-2018\nNorthern Illinois University\nAssociate Professor of Data and Analytics | 2018-present"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/using-xgboost-and-sklearn-to-predict-loan-default/index.en.html",
    "href": "posts/using-xgboost-and-sklearn-to-predict-loan-default/index.en.html",
    "title": "Using xgboost and sklearn to Predict Loan Default",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd \n\nIntroduction\nI wanted to combine xgboost with sklearn pipelines to process a pandas DataFrame. Special thanks to Kaggle user M Yasser H for supplying the Loan Default Dataset.\n\n\nLoad the data\n# Load the data\nloan_df = pd.read_csv(\"../data/Loan_Default.csv\")\n\n\nCreate labels and features\nThe loan status (whether or not the customer defaulted on the loan) is the target variable. We’ll extract that out as our labels. The other columns (minus the ID) will serve as our features. And we’ll take a peek at our features.\n# Split out labels and features, encode labels as integers\ny = loan_df['Status']\n\nX = loan_df.loc[:,~loan_df.columns.isin(['ID','Status'])]\n\n\nSimple data exploration\nTake a peek as our feature variables.\nX.head()\n##    year loan_limit             Gender  ... Region Security_Type dtir1\n## 0  2019         cf  Sex Not Available  ...  south        direct  45.0\n## 1  2019         cf               Male  ...  North        direct   NaN\n## 2  2019         cf               Male  ...  south        direct  46.0\n## 3  2019         cf               Male  ...  North        direct  42.0\n## 4  2019         cf              Joint  ...  North        direct  39.0\n## \n## [5 rows x 32 columns]\nNotice there is a mix of categorical and continuous variables.\nLet’s check if there are any missing values.\nX.isnull().sum()\n## year                             0\n## loan_limit                    3344\n## Gender                           0\n## approv_in_adv                  908\n## loan_type                        0\n## loan_purpose                   134\n## Credit_Worthiness                0\n## open_credit                      0\n## business_or_commercial           0\n## loan_amount                      0\n## rate_of_interest             36439\n## Interest_rate_spread         36639\n## Upfront_charges              39642\n## term                            41\n## Neg_ammortization              121\n## interest_only                    0\n## lump_sum_payment                 0\n## property_value               15098\n## construction_type                0\n## occupancy_type                   0\n## Secured_by                       0\n## total_units                      0\n## income                        9150\n## credit_type                      0\n## Credit_Score                     0\n## co-applicant_credit_type         0\n## age                            200\n## submission_of_application      200\n## LTV                          15098\n## Region                           0\n## Security_Type                    0\n## dtir1                        24121\n## dtype: int64\n\n\nPreprocessing\nSince we have a mix of continuous and categorical variables, we’ll setup an imputers for each type of variable. So, we are going to separate teh continous and the categorical varabiles into separate DataFrames.\nFor the continuous variables, we’ll impute the median.\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\n\n# extract numeric columns\nnumeric_mask = (X.dtypes != object)\nnumeric_columns = X.columns[numeric_mask].tolist()\nnumeric_df = X[numeric_columns]\n\n# create \"imputer\", just going to fill missing values with \"missing\"\nnumeric_imputor = DataFrameMapper(\n  [([numeric_feature], SimpleImputer(strategy='median')) for numeric_feature in numeric_df],\n  input_df=True,\n  df_out=True\n  )\nFor the categorical variables, we’ll impute the value ‘missing’.\n# extract categorical features\ncategorical_mask = (X.dtypes == object)\ncategorical_columns = X.columns[categorical_mask].tolist()\ncategorical_df = X[categorical_columns]\n\ncategorical_imputor = DataFrameMapper(\n  [([categorical_feature], SimpleImputer(strategy='constant', fill_value = \"missing\")) for categorical_feature in categorical_df],\n  input_df=True,\n  df_out=True\n  )\n\n\nBuild the pipeline\nWe are going to use sklearn’s DictVectorizer, which operates on numpy arrays/matrices. So to make it compatiable with DataFrames, we’ll create a simple utility class to allow a DataFrame to be passed through the pipeline. Thanks to Chanseok for the dictifier code.\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define Dictifier class to turn df into dictionary as part of pipeline\nclass Dictifier(BaseEstimator, TransformerMixin):\n  def fit(self, X, y=None):\n    return self\n  \n  def transform(self, X):\n    if type(X) == pd.core.frame.DataFrame:\n      return X.to_dict(\"records\")\n    else:\n      return pd.DataFrame(X).to_dict(\"records\")\nNow we build the pipeline. Notice how we use the FeatureUnion to bring the continous and categorical features back together again at the start of the pipeline.\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nimport xgboost as xgb\n\nimputed_df = FeatureUnion([\n  ('num_imputer', numeric_imputor),\n  ('cat_imputer', categorical_imputor)    \n  ])\n  \nxgb_pipeline = Pipeline([\n  (\"featureunion\", imputed_df),\n  ('dictifier', Dictifier()),\n  ('dict_vectorizer', DictVectorizer(sort=False)),\n  (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n  ])\n\n\n3-Fold Cross Validation\nWe’ll use 3-fold cross validation (instead of 10, or something greater) to minimize compute time.\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb_pipeline, X, y, scoring=\"f1\", cv=3)\navg_f1 = np.mean(np.sqrt(np.abs(scores)))\nprint(\"Avg F1 Score:\", avg_f1)\n## Avg F1 Score: 0.9999863537583441\n\n\nConclusions\nThe average F1 score is suspiciously high, so let’s not put much clout in the quality of the model. But it serves the purpose of demonstrating how to pass a ‘pandas’ DataFrame through an sklearn pipeline, preprocess mixed variable (continuous and categorical) data, and build an xgboost classifier.\nJust because it’s bugging me, here are a few things that may need to be improved in this model:\n\nThere is probably a high correlation between the target variable and some feature variables. We can check this quickly. Fille the NAs with zero and correlate it with the loan status (which is 0 and 1).\n\nnumeric_fillna_df = numeric_df.fillna(0)\nnumeric_fillna_df.corrwith(y)\n## year                         NaN\n## loan_amount            -0.036825\n## rate_of_interest       -0.958875\n## Interest_rate_spread   -0.392977\n## Upfront_charges        -0.431183\n## term                   -0.000675\n## property_value         -0.273267\n## income                 -0.044620\n## Credit_Score            0.004004\n## LTV                    -0.267700\n## dtir1                  -0.325613\n## dtype: float64\nWe can see that rate_of_interest has a high inverse correlation with the target variable. However, I did try removing rate_of_interest and still ended up with an F1 score of 0.9999.\n\nFind a better way to impute the missing categorical. Chirag Goyal enumerates some options in this post. I suspect that building a model to predict missing values would be an option. Another simpler option would be to just randomly insert existing values. But, currently, with the imputer in this post, it is essentially treating ‘missing’ as a legit value."
  },
  {
    "objectID": "posts/using-xgboost-and-sklearn-to-predict-loan-default/index.html",
    "href": "posts/using-xgboost-and-sklearn-to-predict-loan-default/index.html",
    "title": "Using xgboost and sklearn to Predict Loan Default",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd \n\n\nIntroduction\nI wanted to combine xgboost with sklearn pipelines to process a pandas DataFrame. Special thanks to Kaggle user M Yasser H for supplying the Loan Default Dataset.\n\n\nLoad the data\n\n# Load the data\nloan_df = pd.read_csv(\"Loan_Default.csv\")\n\n\n\nCreate labels and features\nThe loan status (whether or not the customer defaulted on the loan) is the target variable. We’ll extract that out as our labels. The other columns (minus the ID) will serve as our features. And we’ll take a peek at our features.\n\n# Split out labels and features, encode labels as integers\ny = loan_df['Status']\n\nX = loan_df.loc[:,~loan_df.columns.isin(['ID','Status'])]\n\n\n\nSimple data exploration\nTake a peek as our feature variables.\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      year\n      loan_limit\n      Gender\n      approv_in_adv\n      loan_type\n      loan_purpose\n      Credit_Worthiness\n      open_credit\n      business_or_commercial\n      loan_amount\n      ...\n      income\n      credit_type\n      Credit_Score\n      co-applicant_credit_type\n      age\n      submission_of_application\n      LTV\n      Region\n      Security_Type\n      dtir1\n    \n  \n  \n    \n      0\n      2019\n      cf\n      Sex Not Available\n      nopre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      116500\n      ...\n      1740.0\n      EXP\n      758\n      CIB\n      25-34\n      to_inst\n      98.728814\n      south\n      direct\n      45.0\n    \n    \n      1\n      2019\n      cf\n      Male\n      nopre\n      type2\n      p1\n      l1\n      nopc\n      b/c\n      206500\n      ...\n      4980.0\n      EQUI\n      552\n      EXP\n      55-64\n      to_inst\n      NaN\n      North\n      direct\n      NaN\n    \n    \n      2\n      2019\n      cf\n      Male\n      pre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      406500\n      ...\n      9480.0\n      EXP\n      834\n      CIB\n      35-44\n      to_inst\n      80.019685\n      south\n      direct\n      46.0\n    \n    \n      3\n      2019\n      cf\n      Male\n      nopre\n      type1\n      p4\n      l1\n      nopc\n      nob/c\n      456500\n      ...\n      11880.0\n      EXP\n      587\n      CIB\n      45-54\n      not_inst\n      69.376900\n      North\n      direct\n      42.0\n    \n    \n      4\n      2019\n      cf\n      Joint\n      pre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      696500\n      ...\n      10440.0\n      CRIF\n      602\n      EXP\n      25-34\n      not_inst\n      91.886544\n      North\n      direct\n      39.0\n    \n  \n\n5 rows × 32 columns\n\n\n\nNotice there is a mix of categorical and continuous variables.\nLet’s check if there are any missing values.\n\nX.isnull().sum()\n\nyear                             0\nloan_limit                    3344\nGender                           0\napprov_in_adv                  908\nloan_type                        0\nloan_purpose                   134\nCredit_Worthiness                0\nopen_credit                      0\nbusiness_or_commercial           0\nloan_amount                      0\nrate_of_interest             36439\nInterest_rate_spread         36639\nUpfront_charges              39642\nterm                            41\nNeg_ammortization              121\ninterest_only                    0\nlump_sum_payment                 0\nproperty_value               15098\nconstruction_type                0\noccupancy_type                   0\nSecured_by                       0\ntotal_units                      0\nincome                        9150\ncredit_type                      0\nCredit_Score                     0\nco-applicant_credit_type         0\nage                            200\nsubmission_of_application      200\nLTV                          15098\nRegion                           0\nSecurity_Type                    0\ndtir1                        24121\ndtype: int64\n\n\n\n\nPreprocessing\nSince we have a mix of continuous and categorical variables, we’ll setup an imputers for each type of variable. So, we are going to separate teh continous and the categorical varabiles into separate DataFrames.\nFor the continuous variables, we’ll impute the median.\n\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\n\n# extract numeric columns\nnumeric_mask = (X.dtypes != object)\nnumeric_columns = X.columns[numeric_mask].tolist()\nnumeric_df = X[numeric_columns]\n\n# create \"imputer\", just going to fill missing values with \"missing\"\nnumeric_imputor = DataFrameMapper(\n  [([numeric_feature], SimpleImputer(strategy='median')) for numeric_feature in numeric_df],\n  input_df=True,\n  df_out=True\n  )\n\nFor the categorical variables, we’ll impute the value ‘missing’.\n\n# extract categorical features\ncategorical_mask = (X.dtypes == object)\ncategorical_columns = X.columns[categorical_mask].tolist()\ncategorical_df = X[categorical_columns]\n\ncategorical_imputor = DataFrameMapper(\n  [([categorical_feature], SimpleImputer(strategy='constant', fill_value = \"missing\")) for categorical_feature in categorical_df],\n  input_df=True,\n  df_out=True\n  )\n\n\n\nBuild the pipeline\nWe are going to use sklearn’s DictVectorizer, which operates on numpy arrays/matrices. So to make it compatiable with DataFrames, we’ll create a simple utility class to allow a DataFrame to be passed through the pipeline. Thanks to Chanseok for the dictifier code.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define Dictifier class to turn df into dictionary as part of pipeline\nclass Dictifier(BaseEstimator, TransformerMixin):\n  def fit(self, X, y=None):\n    return self\n  \n  def transform(self, X):\n    if type(X) == pd.core.frame.DataFrame:\n      return X.to_dict(\"records\")\n    else:\n      return pd.DataFrame(X).to_dict(\"records\")\n\nNow we build the pipeline. Notice how we use the FeatureUnion to bring the continous and categorical features back together again at the start of the pipeline.\n\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nimport xgboost as xgb\n\nimputed_df = FeatureUnion([\n  ('num_imputer', numeric_imputor),\n  ('cat_imputer', categorical_imputor)    \n  ])\n  \nxgb_pipeline = Pipeline([\n  (\"featureunion\", imputed_df),\n  ('dictifier', Dictifier()),\n  ('dict_vectorizer', DictVectorizer(sort=False)),\n  (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n  ])\n\nC:\\Users\\A1849763\\Anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning:\n\npandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n\n\n\n\n\n3-Fold Cross Validation\nWe’ll use 3-fold cross validation (instead of 10, or something greater) to minimize compute time.\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb_pipeline, X, y, scoring=\"f1\", cv=3)\navg_f1 = np.mean(np.sqrt(np.abs(scores)))\nprint(\"Avg F1 Score:\", avg_f1)\n\nAvg F1 Score: 0.9999863537583441\n\n\n\n\nConclusions\nThe average F1 score is suspiciously high, so let’s not put much clout in the quality of the model. But it serves the purpose of demonstrating how to pass a ‘pandas’ DataFrame through an sklearn pipeline, preprocess mixed variable (continuous and categorical) data, and build an xgboost classifier.\nJust because it’s bugging me, here are a few things that may need to be improved in this model:\n\nThere is probably a high correlation between the target variable and some feature variables. We can check this quickly. Fille the NAs with zero and correlate it with the loan status (which is 0 and 1).\n\n\nnumeric_fillna_df = numeric_df.fillna(0)\nnumeric_fillna_df.corrwith(y)\n\nyear                         NaN\nloan_amount            -0.036825\nrate_of_interest       -0.958875\nInterest_rate_spread   -0.392977\nUpfront_charges        -0.431183\nterm                   -0.000675\nproperty_value         -0.273267\nincome                 -0.044620\nCredit_Score            0.004004\nLTV                    -0.267700\ndtir1                  -0.325613\ndtype: float64\n\n\nWe can see that rate_of_interest has a high inverse correlation with the target variable. However, I did try removing rate_of_interest and still ended up with an F1 score of 0.9999.\n\nFind a better way to impute the missing categorical. Chirag Goyal enumerates some options in this post. I suspect that building a model to predict missing values would be an option. Another simpler option would be to just randomly insert existing values. But, currently, with the imputer in this post, it is essentially treating ‘missing’ as a legit value."
  },
  {
    "objectID": "posts/regex-preserve_currency_tokens/index.html",
    "href": "posts/regex-preserve_currency_tokens/index.html",
    "title": "regex to Preserve Currency Tokens",
    "section": "",
    "text": "Introduction\nI’ve been working to collect a corpus of tax legislation, regulations, and court cases. At the core of my corpus is the United States Code Title 26 (USC26), otherwise known as the Internal Revenue Code or the U.S. Tax Code. I’m using the corpus to train a tax-specific word embedding. Word embeddings capture similarity semantics of tokens (words, or multi-word phrases) by encoding them into a n-dimensional space. Similar tokens are embedded into the n-dimensional space in close proximity.\nSince currency values may have important semantic meaning in the USC–for instance, tax bracket boundaries–I want to preserve them so the tokenizer does not break them apart. To do this, I want to replace spaces, commas, decimals, and dollar signs with underscores. Here are some examples:\n\n\n\nOriginal\nPreserved\n\n\n\n\n$172,175\n172_175_usd\n\n\n$2.5 million\n2_5_million_usd\n\n\n$56,750.25\n56_750_25_usd\n\n\n\n\n\nImplementation\nThe approach I took was to first match the different currency formats and then pass the match to a helper function that would replace spaces, commas, decimals, and dollar signs with underscores.\nHere is the pattern I created to match currency tokens:\n\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n\n\n\\$ matches a dollar sign.\n\\d{0,3} The number after the dollar sign and before a comma or decimal boundary. Looking for 0 to 3 digits allows for these patterns: $.75, $125,000, etc.\n(\\,\\d{3}){0,4}(\\.\\d{1,2})? matches repeated comma boundaries (if they exist) and two-decimal places for cents (if they exist). More specifically:\n\n(\\,\\d{3}) matches “,xxx”. With the addition of {0,4} it matches “,xxx” repeatedly–up to trillions (which is probably overkill for the USC26 because most large currency values have “million” or “billion” text suffixes).\n(\\.\\d{1,2})? matches an optional cents.\n\n( (million|billion))? just checks for a text suffix (the ? at the end makes it optional.)\n\nNext, here is a simple helper function to replace spaces, commas, decimals, and dollar signs with underscores and tack on “_usd” at the end. Python’s string replace() method is faster than the regex sub(), so I went with replace().\n\nimport re\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n\nNow I need a test string.\n\ntest_string = \"I made $755.34 billion this year, $5.34 million last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45 in case that's a more realistic salary in my life.\"\n\nThen I compile the regex and replace matches. Notice that the second argument of sub() can take a method that returns a string. I leverage this to call the helper function.\n\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n\n\"I made 755_34_billion_usd this year, 5_34_million_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd in case that's a more realistic salary in my life.\"\n\n\nHere is the full Python implementation.\n\nimport time\nimport timeit\n\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n\ntest_string = \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n\n\n\nR Implementation\nI was curious if python was that much faster than R. So, here is an R implementation. Like Python’s sub(), the str_replace_all function can take a helper function.\n\nlibrary(stringr)\n\npattern <- '\\\\$\\\\d{0,3}(\\\\,\\\\d{3}){0,4}(\\\\.\\\\d{1,2})?( (million|billion))?'\n\nreplace_currency_parts <- function(match) {\n  \n  text <- match %>% \n    str_replace_all(\" \", \"_\") %>% \n    str_replace_all(\"\\\\.\", \"_\") %>% \n    str_replace_all(\"\\\\,\", \"_\")\n    \n  if (str_sub(text, 1, 1) == \"$\"){\n    text <- str_c(str_sub(text, 2), \"_usd\")\n  }\n    \n  return(text)\n}\n\ntest_string <-  \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\nstr_replace_all(test_string, pattern, replace_currency_parts)\n\n[1] \"I made 355_34_million_usd this year, 435_34_billion_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd.\"\n\n\n\n\nWhich is faster?\nSo, which is faster?\nI compare the Python and R implementations by taking the average of 1000 executions of the code. Since the {stringr} package in R is higher level–there’s no option to compile the regex ahead of time and keep it out of the loop–I placed the Python regex compile code inside the loop to make a more fair comparison. In both cases, the units are milliseconds. NOTE: The python time() method returns seconds, so I convert.\n\nfrom time import time\n\nstart_ms = int(round(time() * 1000))\nfor i in range(1000):\n  compiled_pattern = re.compile(pattern)\n  str = compiled_pattern.sub(replace_currency_parts, test_string)\nend_ms = int(round(time() * 1000))\n\nprint(\"elapsed ms = \", (end_ms-start_ms)/1000)\n\nelapsed ms =  0.021\n\n\n\nlibrary(microbenchmark)\n\nmicrobenchmark::microbenchmark(str_replace_all(test_string, pattern, replace_currency_parts), times = 1000)\n\nUnit: milliseconds\n                                                          expr    min      lq\n str_replace_all(test_string, pattern, replace_currency_parts) 1.9412 2.00845\n     mean median     uq    max neval\n 2.181681 2.0652 2.2058 9.9199  1000\n\n\nI don’t know what overhead is included in the {stringr} package, but it’s about two orders of magnitude slower than Python. I suspect it is because the Python replace() string method is quite speedy."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Simple Feature Selection Using a Low-to-No Variance Mask\n\n\n\n\n\n\n\n\n\n\nDemonstrates how to remove features with low to no variance.\n\n\n\n\n\n\nJan 5, 2023\n\n\nMatt Pickard\n\n\n\n\n\n\n\n\nregex to Preserve Currency Tokens\n\n\n\n\n\n\n\nregex\n\n\ndata preparation\n\n\npython\n\n\nr\n\n\n\n\nDevelopment of a regex to preserve currency values in the U.S. Tax Code.\n\n\n\n\n\n\nMar 14, 2022\n\n\nMatt Pickard\n\n\n\n\n\n\n\n\nUsing xgboost and sklearn to Predict Loan Default\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2022\n\n\nMatt Pickard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "A place to showcase my projects."
  },
  {
    "objectID": "blog/posts/using-xgboost-and-sklearn-to-predict-loan-default.html",
    "href": "blog/posts/using-xgboost-and-sklearn-to-predict-loan-default.html",
    "title": "Using xgboost and sklearn to Predict Loan Default",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd \n\n\nIntroduction\nI wanted to combine xgboost with sklearn pipelines to process a pandas DataFrame. Special thanks to Kaggle user M Yasser H for supplying the Loan Default Dataset.\n\n\nLoad the data\n\n# Load the data\nloan_df = pd.read_csv(\"data/Loan_Default.csv\")\n\n\n\nCreate labels and features\nThe loan status (whether or not the customer defaulted on the loan) is the target variable. We’ll extract that out as our labels. The other columns (minus the ID) will serve as our features. And we’ll take a peek at our features.\n\n# Split out labels and features, encode labels as integers\ny = loan_df['Status']\n\nX = loan_df.loc[:,~loan_df.columns.isin(['ID','Status'])]\n\n\n\nSimple data exploration\nTake a peek at our feature variables.\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      year\n      loan_limit\n      Gender\n      approv_in_adv\n      loan_type\n      loan_purpose\n      Credit_Worthiness\n      open_credit\n      business_or_commercial\n      loan_amount\n      ...\n      income\n      credit_type\n      Credit_Score\n      co-applicant_credit_type\n      age\n      submission_of_application\n      LTV\n      Region\n      Security_Type\n      dtir1\n    \n  \n  \n    \n      0\n      2019\n      cf\n      Sex Not Available\n      nopre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      116500\n      ...\n      1740.0\n      EXP\n      758\n      CIB\n      25-34\n      to_inst\n      98.728814\n      south\n      direct\n      45.0\n    \n    \n      1\n      2019\n      cf\n      Male\n      nopre\n      type2\n      p1\n      l1\n      nopc\n      b/c\n      206500\n      ...\n      4980.0\n      EQUI\n      552\n      EXP\n      55-64\n      to_inst\n      NaN\n      North\n      direct\n      NaN\n    \n    \n      2\n      2019\n      cf\n      Male\n      pre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      406500\n      ...\n      9480.0\n      EXP\n      834\n      CIB\n      35-44\n      to_inst\n      80.019685\n      south\n      direct\n      46.0\n    \n    \n      3\n      2019\n      cf\n      Male\n      nopre\n      type1\n      p4\n      l1\n      nopc\n      nob/c\n      456500\n      ...\n      11880.0\n      EXP\n      587\n      CIB\n      45-54\n      not_inst\n      69.376900\n      North\n      direct\n      42.0\n    \n    \n      4\n      2019\n      cf\n      Joint\n      pre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      696500\n      ...\n      10440.0\n      CRIF\n      602\n      EXP\n      25-34\n      not_inst\n      91.886544\n      North\n      direct\n      39.0\n    \n  \n\n5 rows × 32 columns\n\n\n\nNotice there is a mix of categorical and continuous variables.\nLet’s check if there are any missing values.\n\nX.isnull().sum()\n\nyear                             0\nloan_limit                    3344\nGender                           0\napprov_in_adv                  908\nloan_type                        0\nloan_purpose                   134\nCredit_Worthiness                0\nopen_credit                      0\nbusiness_or_commercial           0\nloan_amount                      0\nrate_of_interest             36439\nInterest_rate_spread         36639\nUpfront_charges              39642\nterm                            41\nNeg_ammortization              121\ninterest_only                    0\nlump_sum_payment                 0\nproperty_value               15098\nconstruction_type                0\noccupancy_type                   0\nSecured_by                       0\ntotal_units                      0\nincome                        9150\ncredit_type                      0\nCredit_Score                     0\nco-applicant_credit_type         0\nage                            200\nsubmission_of_application      200\nLTV                          15098\nRegion                           0\nSecurity_Type                    0\ndtir1                        24121\ndtype: int64\n\n\n\n\nPreprocessing\nSince we have a mix of continuous and categorical variables, we’ll set up an imputer for each type of variable. So, we are going to separate the continuous and the categorical variables into separate DataFrames.\nFor the continuous variables, we’ll impute the median.\n\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\n\n# extract numeric columns\nnumeric_mask = (X.dtypes != object)\nnumeric_columns = X.columns[numeric_mask].tolist()\nnumeric_df = X[numeric_columns]\n\n# create \"imputer\", just going to fill missing values with \"missing\"\nnumeric_imputor = DataFrameMapper(\n  [([numeric_feature], SimpleImputer(strategy='median')) for numeric_feature in numeric_df],\n  input_df=True,\n  df_out=True\n  )\n\nFor the categorical variables, we’ll impute the value ‘missing’.\n\n# extract categorical features\ncategorical_mask = (X.dtypes == object)\ncategorical_columns = X.columns[categorical_mask].tolist()\ncategorical_df = X[categorical_columns]\n\ncategorical_imputor = DataFrameMapper(\n  [([categorical_feature], SimpleImputer(strategy='constant', fill_value = \"missing\")) for categorical_feature in categorical_df],\n  input_df=True,\n  df_out=True\n  )\n\n\n\nBuild the pipeline\nWe are going to use sklearn’s DictVectorizer, which operates on numpy arrays/matrices. So to make it compatible with DataFrames, we’ll create a simple utility class to allow a DataFrame to be passed through the pipeline. Thanks to Chanseok for the dictifier code.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define Dictifier class to turn df into a dictionary as part of the pipeline\nclass Dictifier(BaseEstimator, TransformerMixin):\n  def fit(self, X, y=None):\n    return self\n  \n  def transform(self, X):\n    if type(X) == pd.core.frame.DataFrame:\n      return X.to_dict(\"records\")\n    else:\n      return pd.DataFrame(X).to_dict(\"records\")\n\nNow we build the pipeline. Notice how we use the FeatureUnion to bring the continuous and categorical features back together again at the start of the pipeline.\n\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nimport xgboost as xgb\n\nimputed_df = FeatureUnion([\n  ('num_imputer', numeric_imputor),\n  ('cat_imputer', categorical_imputor)    \n  ])\n  \nxgb_pipeline = Pipeline([\n  (\"featureunion\", imputed_df),\n  ('dictifier', Dictifier()),\n  ('dict_vectorizer', DictVectorizer(sort=False)),\n  (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n  ])\n\nC:\\Users\\A1849763\\Anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning:\n\npandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n\n\n\n\n\n3-Fold Cross Validation\nWe’ll use 3-fold cross-validation (instead of 10, or something greater) to minimize compute time.\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb_pipeline, X, y, scoring=\"f1\", cv=3)\navg_f1 = np.mean(np.sqrt(np.abs(scores)))\nprint(\"Avg F1 Score:\", avg_f1)\n\nAvg F1 Score: 0.9999863537583441\n\n\n\n\nConclusions\nThe average F1 score is suspiciously high, so let’s not put much clout on the quality of the model. But it serves the purpose of demonstrating how to pass a ‘pandas’ DataFrame through a sklearn pipeline, preprocess mixed variable (continuous and categorical) data, and build an xgboost classifier.\nJust because it’s bugging me, here are a few things that may need to be improved in this model:\n\nThere is probably a high correlation between the target variable and some feature variables. We can check this quickly. Fill the NAs with zero and correlate it with the loan status (which is 0 and 1).\n\n\nnumeric_fillna_df = numeric_df.fillna(0)\nnumeric_fillna_df.corrwith(y)\n\nyear                         NaN\nloan_amount            -0.036825\nrate_of_interest       -0.958875\nInterest_rate_spread   -0.392977\nUpfront_charges        -0.431183\nterm                   -0.000675\nproperty_value         -0.273267\nincome                 -0.044620\nCredit_Score            0.004004\nLTV                    -0.267700\ndtir1                  -0.325613\ndtype: float64\n\n\nWe can see that rate_of_interest has a high inverse correlation with the target variable. However, I did try removing rate_of_interest and still ended up with an F1 score of 0.9999.\n\nFind a better way to impute the missing categorical. Chirag Goyal enumerates some options in this post. I suspect that building a model to predict missing values would be an option. Another simpler option would be to just randomly insert existing values. But, currently, with the imputer in this post, it is essentially treating ‘missing’ as a legit value."
  },
  {
    "objectID": "blog/posts/regex-preserve_currency_tokens.html",
    "href": "blog/posts/regex-preserve_currency_tokens.html",
    "title": "regex to Preserve Currency Tokens",
    "section": "",
    "text": "Introduction\nI’ve been working to collect a corpus of tax legislation, regulations, and court cases. At the core of my corpus is the United States Code Title 26 (USC26), otherwise known as the Internal Revenue Code or the U.S. Tax Code. I’m using the corpus to train a tax-specific word embedding. Word embeddings capture similarity semantics of tokens (words, or multi-word phrases) by encoding them into a n-dimensional space. Similar tokens are embedded into the n-dimensional space in close proximity.\nSince currency values may have important semantic meaning in the USC–for instance, tax bracket boundaries–I want to preserve them so the tokenizer does not break them apart. To do this, I want to replace spaces, commas, decimals, and dollar signs with underscores. Here are some examples:\n\nExamples of Currency Values\n\n\nOriginal\nPreserved\n\n\n\n\n$172,175\n172_175_usd\n\n\n$2.5 million\n2_5_million_usd\n\n\n$56,750.25\n56_750_25_usd\n\n\n\n\n\nImplementation\nThe approach I took was to first match the different currency formats and then pass the match to a helper function that would replace spaces, commas, decimals, and dollar signs with underscores.\nHere is the pattern I created to match currency tokens:\n\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n\n\n\\$ matches a dollar sign.\n\\d{0,3} The number after the dollar sign and before a comma or decimal boundary. Looking for 0 to 3 digits allows for these patterns: $.75, $125,000, etc.\n(\\,\\d{3}){0,4}(\\.\\d{1,2})? matches repeated comma boundaries (if they exist) and two-decimal places for cents (if they exist). More specifically:\n\n(\\,\\d{3}) matches “,xxx”. With the addition of {0,4} it matches “,xxx” repeatedly–up to trillions (which is probably overkill for the USC26 because most large currency values have “million” or “billion” text suffixes).\n(\\.\\d{1,2})? matches an optional cents.\n\n( (million|billion))? just checks for a text suffix (the ? at the end makes it optional.)\n\nNext, here is a simple helper function to replace spaces, commas, decimals, and dollar signs with underscores and tack on “_usd” at the end. Python’s string replace() method is faster than the regex sub(), so I went with replace().\n\nimport re\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n\nNow I need a test string.\n\ntest_string = \"I made $755.34 billion this year, $5.34 million last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45 in case that's a more realistic salary in my life.\"\n\nThen I compile the regex and replace matches. Notice that the second argument of sub() can take a method that returns a string. I leverage this to call the helper function.\n\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n\n\"I made 755_34_billion_usd this year, 5_34_million_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd in case that's a more realistic salary in my life.\"\n\n\nHere is the full Python implementation.\n\nimport time\nimport timeit\n\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n\ntest_string = \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n\n\n\nR Implementation\nI was curious if python was that much faster than R. So, here is an R implementation. Like Python’s sub(), the str_replace_all function can take a helper function.\n\nlibrary(stringr)\n\npattern <- '\\\\$\\\\d{0,3}(\\\\,\\\\d{3}){0,4}(\\\\.\\\\d{1,2})?( (million|billion))?'\n\nreplace_currency_parts <- function(match) {\n  \n  text <- match %>% \n    str_replace_all(\" \", \"_\") %>% \n    str_replace_all(\"\\\\.\", \"_\") %>% \n    str_replace_all(\"\\\\,\", \"_\")\n    \n  if (str_sub(text, 1, 1) == \"$\"){\n    text <- str_c(str_sub(text, 2), \"_usd\")\n  }\n    \n  return(text)\n}\n\ntest_string <-  \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\nstr_replace_all(test_string, pattern, replace_currency_parts)\n\n[1] \"I made 355_34_million_usd this year, 435_34_billion_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd.\"\n\n\n\n\nWhich is faster?\nSo, which is faster?\nI compare the Python and R implementations by taking the average of 1000 executions of the code. Since the {stringr} package in R is higher level–there’s no option to compile the regex ahead of time and keep it out of the loop–I placed the Python regex compile code inside the loop to make a more fair comparison. In both cases, the units are milliseconds. NOTE: The python time() method returns seconds, so I convert.\n\nfrom time import time\n\nstart_ms = int(round(time() * 1000))\nfor i in range(1000):\n  compiled_pattern = re.compile(pattern)\n  str = compiled_pattern.sub(replace_currency_parts, test_string)\nend_ms = int(round(time() * 1000))\n\nprint(\"elapsed ms = \", (end_ms-start_ms)/1000)\n\nelapsed ms =  0.024\n\n\n\nlibrary(microbenchmark)\n\nmicrobenchmark::microbenchmark(str_replace_all(test_string, pattern, replace_currency_parts), times = 1000)\n\nUnit: milliseconds\n                                                          expr    min     lq\n str_replace_all(test_string, pattern, replace_currency_parts) 1.9911 2.0367\n     mean median     uq     max neval\n 2.100773 2.0643 2.0952 10.0281  1000\n\n\nI don’t know what overhead is included in the {stringr} package, but it’s about two orders of magnitude slower than Python. I suspect it is because the Python replace() string method is quite speedy."
  },
  {
    "objectID": "blog/dataprep/regex-preserve_currency_tokens.html",
    "href": "blog/dataprep/regex-preserve_currency_tokens.html",
    "title": "regex to Preserve Currency Tokens",
    "section": "",
    "text": "Introduction\nI’ve been working to collect a corpus of tax legislation, regulations, and court cases. At the core of my corpus is the United States Code Title 26 (USC26), otherwise known as the Internal Revenue Code or the U.S. Tax Code. I’m using the corpus to train a tax-specific word embedding. Word embeddings capture similarity semantics of tokens (words, or multi-word phrases) by encoding them into a n-dimensional space. Similar tokens are embedded into the n-dimensional space in close proximity.\nSince currency values may have important semantic meaning in the USC–for instance, tax bracket boundaries–I want to preserve them so the tokenizer does not break them apart. To do this, I want to replace spaces, commas, decimals, and dollar signs with underscores. Here are some examples:\n\nExamples of Currency Values\n\n\nOriginal\nPreserved\n\n\n\n\n$172,175\n172_175_usd\n\n\n$2.5 million\n2_5_million_usd\n\n\n$56,750.25\n56_750_25_usd\n\n\n\n\n\nImplementation\nThe approach I took was to first match the different currency formats and then pass the match to a helper function that would replace spaces, commas, decimals, and dollar signs with underscores.\nHere is the pattern I created to match currency tokens:\n\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n\n\n\\$ matches a dollar sign.\n\\d{0,3} The number after the dollar sign and before a comma or decimal boundary. Looking for 0 to 3 digits allows for these patterns: $.75, $125,000, etc.\n(\\,\\d{3}){0,4}(\\.\\d{1,2})? matches repeated comma boundaries (if they exist) and two-decimal places for cents (if they exist). More specifically:\n\n(\\,\\d{3}) matches “,xxx”. With the addition of {0,4} it matches “,xxx” repeatedly–up to trillions (which is probably overkill for the USC26 because most large currency values have “million” or “billion” text suffixes).\n(\\.\\d{1,2})? matches an optional cents.\n\n( (million|billion))? just checks for a text suffix (the ? at the end makes it optional.)\n\nNext, here is a simple helper function to replace spaces, commas, decimals, and dollar signs with underscores and tack on “_usd” at the end. Python’s string replace() method is faster than the regex sub(), so I went with replace().\n\nimport re\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n\nNow I need a test string.\n\ntest_string = \"I made $755.34 billion this year, $5.34 million last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45 in case that's a more realistic salary in my life.\"\n\nThen I compile the regex and replace matches. Notice that the second argument of sub() can take a method that returns a string. I leverage this to call the helper function.\n\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n\n\"I made 755_34_billion_usd this year, 5_34_million_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd in case that's a more realistic salary in my life.\"\n\n\nHere is the full Python implementation.\n\nimport time\nimport timeit\n\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n\ntest_string = \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n\n\n\nR Implementation\nI was curious if python was that much faster than R. So, here is an R implementation. Like Python’s sub(), the str_replace_all function can take a helper function.\n\nlibrary(stringr)\n\npattern <- '\\\\$\\\\d{0,3}(\\\\,\\\\d{3}){0,4}(\\\\.\\\\d{1,2})?( (million|billion))?'\n\nreplace_currency_parts <- function(match) {\n  \n  text <- match %>% \n    str_replace_all(\" \", \"_\") %>% \n    str_replace_all(\"\\\\.\", \"_\") %>% \n    str_replace_all(\"\\\\,\", \"_\")\n    \n  if (str_sub(text, 1, 1) == \"$\"){\n    text <- str_c(str_sub(text, 2), \"_usd\")\n  }\n    \n  return(text)\n}\n\ntest_string <-  \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\nstr_replace_all(test_string, pattern, replace_currency_parts)\n\n[1] \"I made 355_34_million_usd this year, 435_34_billion_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd.\"\n\n\n\n\nWhich is faster?\nSo, which is faster?\nI compare the Python and R implementations by taking the average of 1000 executions of the code. Since the {stringr} package in R is higher level–there’s no option to compile the regex ahead of time and keep it out of the loop–I placed the Python regex compile code inside the loop to make a more fair comparison. In both cases, the units are milliseconds. NOTE: The python time() method returns seconds, so I convert.\n\nfrom time import time\n\nstart_ms = int(round(time() * 1000))\nfor i in range(1000):\n  compiled_pattern = re.compile(pattern)\n  str = compiled_pattern.sub(replace_currency_parts, test_string)\nend_ms = int(round(time() * 1000))\n\nprint(\"elapsed ms = \", (end_ms-start_ms)/1000)\n\nelapsed ms =  0.02\n\n\n\nlibrary(microbenchmark)\n\nmicrobenchmark::microbenchmark(str_replace_all(test_string, pattern, replace_currency_parts), times = 1000)\n\nUnit: milliseconds\n                                                          expr    min      lq\n str_replace_all(test_string, pattern, replace_currency_parts) 1.8425 1.91115\n     mean median      uq     max neval\n 2.003461 1.9451 2.00055 10.2409  1000\n\n\nI don’t know what overhead is included in the {stringr} package, but it’s about two orders of magnitude slower than Python. I suspect it is because the Python replace() string method is quite speedy."
  },
  {
    "objectID": "blog/machinelearn/using-xgboost-and-sklearn-to-predict-loan-default.html",
    "href": "blog/machinelearn/using-xgboost-and-sklearn-to-predict-loan-default.html",
    "title": "Using xgboost and sklearn to Predict Loan Default",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd \n\n\nIntroduction\nI wanted to combine xgboost with sklearn pipelines to process a pandas DataFrame. Special thanks to Kaggle user M Yasser H for supplying the Loan Default Dataset.\n\n\nLoad the data\n\n# Load the data\nloan_df = pd.read_csv(\"data/Loan_Default.csv\")\n\n\n\nCreate labels and features\nThe loan status (whether or not the customer defaulted on the loan) is the target variable. We’ll extract that out as our labels. The other columns (minus the ID) will serve as our features. And we’ll take a peek at our features.\n\n# Split out labels and features, encode labels as integers\ny = loan_df['Status']\n\nX = loan_df.loc[:,~loan_df.columns.isin(['ID','Status'])]\n\n\n\nSimple data exploration\nTake a peek at our feature variables.\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      year\n      loan_limit\n      Gender\n      approv_in_adv\n      loan_type\n      loan_purpose\n      Credit_Worthiness\n      open_credit\n      business_or_commercial\n      loan_amount\n      ...\n      income\n      credit_type\n      Credit_Score\n      co-applicant_credit_type\n      age\n      submission_of_application\n      LTV\n      Region\n      Security_Type\n      dtir1\n    \n  \n  \n    \n      0\n      2019\n      cf\n      Sex Not Available\n      nopre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      116500\n      ...\n      1740.0\n      EXP\n      758\n      CIB\n      25-34\n      to_inst\n      98.728814\n      south\n      direct\n      45.0\n    \n    \n      1\n      2019\n      cf\n      Male\n      nopre\n      type2\n      p1\n      l1\n      nopc\n      b/c\n      206500\n      ...\n      4980.0\n      EQUI\n      552\n      EXP\n      55-64\n      to_inst\n      NaN\n      North\n      direct\n      NaN\n    \n    \n      2\n      2019\n      cf\n      Male\n      pre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      406500\n      ...\n      9480.0\n      EXP\n      834\n      CIB\n      35-44\n      to_inst\n      80.019685\n      south\n      direct\n      46.0\n    \n    \n      3\n      2019\n      cf\n      Male\n      nopre\n      type1\n      p4\n      l1\n      nopc\n      nob/c\n      456500\n      ...\n      11880.0\n      EXP\n      587\n      CIB\n      45-54\n      not_inst\n      69.376900\n      North\n      direct\n      42.0\n    \n    \n      4\n      2019\n      cf\n      Joint\n      pre\n      type1\n      p1\n      l1\n      nopc\n      nob/c\n      696500\n      ...\n      10440.0\n      CRIF\n      602\n      EXP\n      25-34\n      not_inst\n      91.886544\n      North\n      direct\n      39.0\n    \n  \n\n5 rows × 32 columns\n\n\n\nNotice there is a mix of categorical and continuous variables.\nLet’s check if there are any missing values.\n\nX.isnull().sum()\n\nyear                             0\nloan_limit                    3344\nGender                           0\napprov_in_adv                  908\nloan_type                        0\nloan_purpose                   134\nCredit_Worthiness                0\nopen_credit                      0\nbusiness_or_commercial           0\nloan_amount                      0\nrate_of_interest             36439\nInterest_rate_spread         36639\nUpfront_charges              39642\nterm                            41\nNeg_ammortization              121\ninterest_only                    0\nlump_sum_payment                 0\nproperty_value               15098\nconstruction_type                0\noccupancy_type                   0\nSecured_by                       0\ntotal_units                      0\nincome                        9150\ncredit_type                      0\nCredit_Score                     0\nco-applicant_credit_type         0\nage                            200\nsubmission_of_application      200\nLTV                          15098\nRegion                           0\nSecurity_Type                    0\ndtir1                        24121\ndtype: int64\n\n\n\n\nPreprocessing\nSince we have a mix of continuous and categorical variables, we’ll set up an imputer for each type of variable. So, we are going to separate the continuous and the categorical variables into separate DataFrames.\nFor the continuous variables, we’ll impute the median.\n\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\n\n# extract numeric columns\nnumeric_mask = (X.dtypes != object)\nnumeric_columns = X.columns[numeric_mask].tolist()\nnumeric_df = X[numeric_columns]\n\n# create \"imputer\", just going to fill missing values with \"missing\"\nnumeric_imputor = DataFrameMapper(\n  [([numeric_feature], SimpleImputer(strategy='median')) for numeric_feature in numeric_df],\n  input_df=True,\n  df_out=True\n  )\n\nFor the categorical variables, we’ll impute the value ‘missing’.\n\n# extract categorical features\ncategorical_mask = (X.dtypes == object)\ncategorical_columns = X.columns[categorical_mask].tolist()\ncategorical_df = X[categorical_columns]\n\ncategorical_imputor = DataFrameMapper(\n  [([categorical_feature], SimpleImputer(strategy='constant', fill_value = \"missing\")) for categorical_feature in categorical_df],\n  input_df=True,\n  df_out=True\n  )\n\n\n\nBuild the pipeline\nWe are going to use sklearn’s DictVectorizer, which operates on numpy arrays/matrices. So to make it compatible with DataFrames, we’ll create a simple utility class to allow a DataFrame to be passed through the pipeline. Thanks to Chanseok for the dictifier code.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define Dictifier class to turn df into a dictionary as part of the pipeline\nclass Dictifier(BaseEstimator, TransformerMixin):\n  def fit(self, X, y=None):\n    return self\n  \n  def transform(self, X):\n    if type(X) == pd.core.frame.DataFrame:\n      return X.to_dict(\"records\")\n    else:\n      return pd.DataFrame(X).to_dict(\"records\")\n\nNow we build the pipeline. Notice how we use the FeatureUnion to bring the continuous and categorical features back together again at the start of the pipeline.\n\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nimport xgboost as xgb\n\nimputed_df = FeatureUnion([\n  ('num_imputer', numeric_imputor),\n  ('cat_imputer', categorical_imputor)    \n  ])\n  \nxgb_pipeline = Pipeline([\n  (\"featureunion\", imputed_df),\n  ('dictifier', Dictifier()),\n  ('dict_vectorizer', DictVectorizer(sort=False)),\n  (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n  ])\n\n\n\n3-Fold Cross Validation\nWe’ll use 3-fold cross-validation (instead of 10, or something greater) to minimize compute time.\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb_pipeline, X, y, scoring=\"f1\", cv=3)\navg_f1 = np.mean(np.sqrt(np.abs(scores)))\nprint(\"Avg F1 Score:\", avg_f1)\n\nAvg F1 Score: 0.9999863537583441\n\n\n\n\nConclusions\nThe average F1 score is suspiciously high, so let’s not put much clout on the quality of the model. But it serves the purpose of demonstrating how to pass a ‘pandas’ DataFrame through a sklearn pipeline, preprocess mixed variable (continuous and categorical) data, and build an xgboost classifier.\nJust because it’s bugging me, here are a few things that may need to be improved in this model:\n\nThere is probably a high correlation between the target variable and some feature variables. We can check this quickly. Fill the NAs with zero and correlate it with the loan status (which is 0 and 1).\n\n\nnumeric_fillna_df = numeric_df.fillna(0)\nnumeric_fillna_df.corrwith(y)\n\nyear                         NaN\nloan_amount            -0.036825\nrate_of_interest       -0.958875\nInterest_rate_spread   -0.392977\nUpfront_charges        -0.431183\nterm                   -0.000675\nproperty_value         -0.273267\nincome                 -0.044620\nCredit_Score            0.004004\nLTV                    -0.267700\ndtir1                  -0.325613\ndtype: float64\n\n\nWe can see that rate_of_interest has a high inverse correlation with the target variable. However, I did try removing rate_of_interest and still ended up with an F1 score of 0.9999.\n\nFind a better way to impute the missing categorical. Chirag Goyal enumerates some options in this post. I suspect that building a model to predict missing values would be an option. Another simpler option would be to just randomly insert existing values. But, currently, with the imputer in this post, it is essentially treating ‘missing’ as a legit value."
  },
  {
    "objectID": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html",
    "href": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html",
    "title": "Simple Feature Selection Using a Low-to-No Variance Mask",
    "section": "",
    "text": "One way to perform dimensionality reduction is through feature selection. In this post, we’ll explore how to create a low-variance feature mask (or filter). We’ll cover both a manual approach with dplyr functions and a more automated, production approach with recipes functions. Enjoy!\n\n\nVariance is important in feature selection because of a concept called information gain. Information gain is what we know about one (usually unknown) variable because we can observe another known variable. In supervised learning, we use information from predictor variables to learn something about the target variable.\nTo make this concrete, imagine we want to estimate loan applicants’ creditworthiness. Though we don’t have their credit scores, we do have their monthly income, age, and number of outstanding loans. If every applicant in our data set has three outstanding loans – that is, the number of outstanding loans didn’t vary – then “outstanding loans” does not differentiate loan applicants and, therefore, does not provide any information about the applicants that we can use to estimate their creditworthiness. Therefore, we’d say that number of outstanding loans provides no information gain about creditworthiness.\nWe can remove features with little to no variance because they are not informative. Consider them useless fluff."
  },
  {
    "objectID": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#load-data-with-categorical-variables",
    "href": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#load-data-with-categorical-variables",
    "title": "Simple Feature Selection Using a Low-to-No Variance Mask",
    "section": "Load data with categorical variables",
    "text": "Load data with categorical variables\nTo demonstrate the recipes approach, we’ll load all the features, so we have both continuous and categorical variables.\n\ncredit_df <- \n  read_csv(\"data/credit_data.csv\")\n\nTo make this example interesting, we’ll insert a couple of low-variance features – num_credit_card and num_credit_inquiries.\n\ncredit_df <- credit_df %>% \n  mutate(\n    num_credit_card_rand = runif(n()),\n    num_credit_inquiries_rand = runif(n()),\n    num_credit_card = if_else(\n      num_credit_card_rand < .95, 5, num_credit_card),\n    num_credit_inquiries = if_else(\n      num_credit_inquiries_rand < .95, 3, num_credit_inquiries)\n  ) %>% \n  select(-num_credit_card_rand, -num_credit_inquiries_rand)"
  },
  {
    "objectID": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#inject-some-low-variance-features",
    "href": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#inject-some-low-variance-features",
    "title": "Simple Feature Selection Using a Low-to-No Variance Mask",
    "section": "Inject some low variance features",
    "text": "Inject some low variance features\nTo make this example interesting, we’ll insert a couple of low-variance features – num_credit_card and num_credit_inquiries.\n\ncredit_df <- credit_df %>% \n  mutate(\n    num_credit_card_rand = runif(n()),\n    num_credit_inquiries_rand = runif(n()),\n    num_credit_card = if_else(num_credit_card_rand < .95, 5, num_credit_card),\n    num_credit_inquiries = if_else(num_credit_inquiries_rand < .95, 3, num_credit_inquiries)\n  ) %>% \n  select(-num_credit_card_rand, -num_credit_inquiries_rand)"
  },
  {
    "objectID": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#define-a-recipe-object",
    "href": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#define-a-recipe-object",
    "title": "Simple Feature Selection Using a Low-to-No Variance Mask",
    "section": "Define a recipe object",
    "text": "Define a recipe object\nThen we define a recipe object. Notice the first parameter is a formula. We define credit_score as the target variable and all other features as predictor variables.\nWe add the step_zv() step first. No-variance features will cause problems when we normalize the data with step_scale(). We apply the no-variance step to all predictors and the scale step to only the numeric predictors. Then, we apply step_nzv() to remove low-variance features. prep() “fits” the recipe to the data.\n\nlibrary(recipes) \n\nlow_variance_recipe <- recipe(credit_score ~ ., data = credit_df) %>%\n  step_zv(all_predictors()) %>%\n  step_scale(all_numeric_predictors()) %>%\n  step_nzv(all_predictors()) %>%\n  prep()\n\nWe can use tidy() to peek into the recipe and see the effect it will have on the trained data set it was trained on. Here we look at the third step of the recipe – step_nzv(). We can see that it will remove our two low-variance features – num_credit_card and num_credit_inquiries.\n\ntidy(low_variance_recipe, number = 3)\n\n# A tibble: 2 × 2\n  terms                id       \n  <chr>                <chr>    \n1 num_credit_card      nzv_5xxFQ\n2 num_credit_inquiries nzv_5xxFQ"
  },
  {
    "objectID": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#apply-the-recipe-to-credit_df",
    "href": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#apply-the-recipe-to-credit_df",
    "title": "Simple Feature Selection Using a Low-to-No Variance Mask",
    "section": "Apply the recipe to credit_df",
    "text": "Apply the recipe to credit_df\nIn recipes to apply a trained recipe to a data set, we can use bake(). The new_data parameter allows us to specify the data set “bake” the recipe with. If we pass NULL, it will bake the same data that the recipe was trained on.\n\nfiltered_credit_df <- low_variance_recipe %>% bake(new_data = NULL)\n\nnames(filtered_credit_df)\n\n [1] \"month\"                    \"age\"                     \n [3] \"occupation\"               \"annual_income\"           \n [5] \"monthly_inhand_salary\"    \"num_bank_accounts\"       \n [7] \"interest_rate\"            \"num_of_loan\"             \n [9] \"delay_from_due_date\"      \"num_of_delayed_payment\"  \n[11] \"changed_credit_limit\"     \"outstanding_debt\"        \n[13] \"credit_utilization_ratio\" \"payment_of_min_amount\"   \n[15] \"total_emi_per_month\"      \"amount_invested_monthly\" \n[17] \"payment_behaviour\"        \"monthly_balance\"         \n[19] \"credit_history_months\"    \"credit_score\"            \n\n\nIf we compare the names in the original credit_df to the names in the filtered_credit_df, we can see that num_credit_card and num_credit_inquiries were removed.\n\nsetdiff(names(credit_df), names(filtered_credit_df))\n\n[1] \"num_credit_card\"      \"num_credit_inquiries\""
  },
  {
    "objectID": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#setup",
    "href": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#setup",
    "title": "Simple Feature Selection Using a Low-to-No Variance Mask",
    "section": "Setup",
    "text": "Setup\nFor this example, we’ll use a credit score classification data set from Kaggle, provided by Rohan Paris. The data set is large, so I randomly sampled 20% of it. I also did a little data cleaning. As you’ll see, to make this exercise interesting, we’ll add a few low-variance features to demonstrate feature removal.\nYou can download the cleaned data set here.\nSince variance is conceptually simpler with continuous variables, let’s only load the continuous variables into credit_df.\n\nlibrary(tidyverse)\nlibrary(knitr)\n\ncredit_df <- \n  read_csv(\"data/credit_data.csv\") %>% \n  select_if(is.numeric)\n\nHere’s a peek at the data.\n\nkable(credit_df %>% head(5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nannual_income\nmonthly_inhand_salary\nnum_bank_accounts\nnum_credit_card\ninterest_rate\nnum_of_loan\ndelay_from_due_date\nnum_of_delayed_payment\nchanged_credit_limit\nnum_credit_inquiries\noutstanding_debt\ncredit_utilization_ratio\ntotal_emi_per_month\namount_invested_monthly\nmonthly_balance\ncredit_history_months\n\n\n\n\n41\n16176.83\n1585.070\n8\n3\n10\n3\n17\n14\n0.56\n2445\n1200.70\n29.94733\n21.96219\n38.32601\n358.2188\n241\n\n\n36\n82383.04\n6661.253\n3\n5\n8\n4\n17\n14\n1.71\n2\n1218.57\n33.38963\n162.10896\n123.97997\n630.0364\n392\n\n\n44\n28805.34\n2309.445\n8\n3\n5\n2\n13\n19\n7.02\n0\n796.45\n26.83209\n47.19512\n139.90769\n303.8417\n385\n\n\n28\n45412.95\n3520.412\n8\n6\n30\n6\n28\n21\n23.07\n6\n4601.39\n23.41958\n113.69155\n199.56341\n318.7863\n55\n\n\n45\n17296.38\n1480.365\n6\n10\n30\n5\n21\n17\n17.34\n7\n4624.73\n38.39057\n49.36071\n78.16123\n280.5146\n92"
  },
  {
    "objectID": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#calculate-feature-variances",
    "href": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#calculate-feature-variances",
    "title": "Simple Feature Selection Using a Low-to-No Variance Mask",
    "section": "Calculate feature variances",
    "text": "Calculate feature variances\nTo begin, let’s calculate the variance of each column. With dplyr, we’ll use across() to apply var() to all columns with the everything() selector. Notice that we scale() the data before passing it to var(). It is important to normalize the data so the features are comparable with each other. Unnormalized, num_credit_card and monthly income, have very different variances. To compare their influence on creditworthiness in a fair manner, we normalize them.\nWe use tidyr’s pivot_longer() to pivot the scaled variances to columns. We’ll sort them from largest to smallest.\n\ncredit_variances <- credit_df %>% \n  summarize(\n    across(\n        everything(), \n        ~ var(scale(., center = FALSE)), \n        na.rm = TRUE)) %>% \n  pivot_longer(\n    everything(), \n    names_to = \"feature\", \n    values_to = \"variance\") %>% \n  arrange(desc(variance)) \n\nkable(credit_variances)\n\n\n\n\nfeature\nvariance\n\n\n\n\nnum_of_loan\n0.98558800\n\n\ninterest_rate\n0.98258481\n\n\nnum_of_delayed_payment\n0.98150878\n\n\nnum_credit_inquiries\n0.97843457\n\n\nnum_bank_accounts\n0.97841843\n\n\ntotal_emi_per_month\n0.97569764\n\n\nannual_income\n0.97397017\n\n\nnum_credit_card\n0.97044729\n\n\namount_invested_monthly\n0.90345646\n\n\noutstanding_debt\n0.41753915\n\n\nmonthly_inhand_salary\n0.35255863\n\n\ndelay_from_due_date\n0.33223435\n\n\nchanged_credit_limit\n0.30855481\n\n\nmonthly_balance\n0.22081631\n\n\ncredit_history_months\n0.15974632\n\n\nage\n0.07874258\n\n\ncredit_utilization_ratio\n0.02527138"
  },
  {
    "objectID": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#set-variance-threshold-and-create-a-mask",
    "href": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#set-variance-threshold-and-create-a-mask",
    "title": "Simple Feature Selection Using a Low-to-No Variance Mask",
    "section": "Set variance threshold and create a mask",
    "text": "Set variance threshold and create a mask\nWe can scan down the variances and identify a natural cut-off between amount_invested_monthly and outstanding_debt, however, that would remove too many features. The next cutoff between credit_history_month and age seems more appropriate. So, we can create a variance filter with a threshold of 0.1.\nWe use pull() to get an array of feature names that we can use as a mask.\n\nlow_var_filter <- credit_variances %>% \n  filter(variance < 0.1) %>% \n  pull(feature)\n\nlow_var_filter\n\n[1] \"age\"                      \"credit_utilization_ratio\""
  },
  {
    "objectID": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#apply-the-mask",
    "href": "blog/dataprep/simple-low-variance-feature-selection-mask-in-r.html#apply-the-mask",
    "title": "Simple Feature Selection Using a Low-to-No Variance Mask",
    "section": "Apply the mask",
    "text": "Apply the mask\nWe then apply the mask to the data frame. Notice that it removes ‘age’ and credit_utilization_ratio.\n\nfiltered_credit_df <- credit_df %>% \n  select(-all_of(low_var_filter))\n\nkable(filtered_credit_df %>% head(5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nannual_income\nmonthly_inhand_salary\nnum_bank_accounts\nnum_credit_card\ninterest_rate\nnum_of_loan\ndelay_from_due_date\nnum_of_delayed_payment\nchanged_credit_limit\nnum_credit_inquiries\noutstanding_debt\ntotal_emi_per_month\namount_invested_monthly\nmonthly_balance\ncredit_history_months\n\n\n\n\n16176.83\n1585.070\n8\n3\n10\n3\n17\n14\n0.56\n2445\n1200.70\n21.96219\n38.32601\n358.2188\n241\n\n\n82383.04\n6661.253\n3\n5\n8\n4\n17\n14\n1.71\n2\n1218.57\n162.10896\n123.97997\n630.0364\n392\n\n\n28805.34\n2309.445\n8\n3\n5\n2\n13\n19\n7.02\n0\n796.45\n47.19512\n139.90769\n303.8417\n385\n\n\n45412.95\n3520.412\n8\n6\n30\n6\n28\n21\n23.07\n6\n4601.39\n113.69155\n199.56341\n318.7863\n55\n\n\n17296.38\n1480.365\n6\n10\n30\n5\n21\n17\n17.34\n7\n4624.73\n49.36071\n78.16123\n280.5146\n92"
  }
]