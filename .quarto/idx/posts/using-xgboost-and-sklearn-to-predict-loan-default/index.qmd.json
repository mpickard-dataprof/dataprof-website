{"title":"Using xgboost and sklearn to Predict Loan Default","markdown":{"yaml":{"title":"Using xgboost and sklearn to Predict Loan Default","author":"Matt Pickard","date":"2022-03-01","categories":["python","machine learning"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{python}\nimport numpy as np \nimport pandas as pd \n```\n\nI wanted to combine `xgboost` with `sklearn` pipelines to process a `pandas` DataFrame. Special thanks to Kaggle user M Yasser H for supplying the [Loan Default Dataset](https://www.kaggle.com/yasserh/loan-default-dataset). \n\n# Load the data\n```{python}\n# Load the data\nloan_df = pd.read_csv(\"Loan_Default.csv\")\n```\n\n# Create labels and features\nThe loan status (whether or not the customer defaulted on the loan) is the target variable. We'll extract that out as our labels. The other columns (minus the *ID*) will serve as our features. And we'll take a peek at our features.\n```{python}\n# Split out labels and features, encode labels as integers\ny = loan_df['Status']\n\nX = loan_df.loc[:,~loan_df.columns.isin(['ID','Status'])]\n```\n\n# Simple data exploration\nTake a peek at our feature variables.\n```{python}\nX.head()\n```\n\nNotice there is a mix of categorical and continuous variables.\n\nLet's check if there are any missing values.\n```{python}\nX.isnull().sum()\n```\n\n\n# Preprocessing\n\nSince we have a mix of continuous and categorical variables, we'll set up an imputer for each type of variable. So, we are going to separate the continuous and the categorical variables into separate DataFrames. \n\nFor the continuous variables, we'll impute the median.\n```{python}\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\n\n# extract numeric columns\nnumeric_mask = (X.dtypes != object)\nnumeric_columns = X.columns[numeric_mask].tolist()\nnumeric_df = X[numeric_columns]\n\n# create \"imputer\", just going to fill missing values with \"missing\"\nnumeric_imputor = DataFrameMapper(\n  [([numeric_feature], SimpleImputer(strategy='median')) for numeric_feature in numeric_df],\n  input_df=True,\n  df_out=True\n  )\n```\n\nFor the categorical variables, we'll impute the value 'missing'.\n```{python}\n# extract categorical features\ncategorical_mask = (X.dtypes == object)\ncategorical_columns = X.columns[categorical_mask].tolist()\ncategorical_df = X[categorical_columns]\n\ncategorical_imputor = DataFrameMapper(\n  [([categorical_feature], SimpleImputer(strategy='constant', fill_value = \"missing\")) for categorical_feature in categorical_df],\n  input_df=True,\n  df_out=True\n  )\n```\n\n# Build the pipeline\n\nWe are going to use `sklearn`'s `DictVectorizer`, which operates on numpy arrays/matrices. So to make it compatible with DataFrames, we'll create a simple utility class to allow a DataFrame to be passed through the pipeline. Thanks to Chanseok for the [dictifier code](https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/07/03-Using-XGBoost-in-pipelines.html).\n```{python}\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define Dictifier class to turn df into a dictionary as part of the pipeline\nclass Dictifier(BaseEstimator, TransformerMixin):\n  def fit(self, X, y=None):\n    return self\n  \n  def transform(self, X):\n    if type(X) == pd.core.frame.DataFrame:\n      return X.to_dict(\"records\")\n    else:\n      return pd.DataFrame(X).to_dict(\"records\")\n```\n\nNow we build the pipeline.  Notice how we use the FeatureUnion to bring the continuous and categorical features back together again at the start of the pipeline.\n```{python}\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nimport xgboost as xgb\n\nimputed_df = FeatureUnion([\n  ('num_imputer', numeric_imputor),\n  ('cat_imputer', categorical_imputor)    \n  ])\n  \nxgb_pipeline = Pipeline([\n  (\"featureunion\", imputed_df),\n  ('dictifier', Dictifier()),\n  ('dict_vectorizer', DictVectorizer(sort=False)),\n  (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n  ])\n```\n\n# 3-Fold Cross Validation\nWe'll use 3-fold cross-validation (instead of 10, or something greater) to minimize compute time.\n```{python, warning=FALSE}\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb_pipeline, X, y, scoring=\"f1\", cv=3)\navg_f1 = np.mean(np.sqrt(np.abs(scores)))\nprint(\"Avg F1 Score:\", avg_f1)\n```\n\n# Conclusions\nThe average F1 score is suspiciously high, so let's not put much clout on the quality of the model. But it serves the purpose of demonstrating how to pass a 'pandas' DataFrame through a `sklearn` pipeline, preprocess mixed variable (continuous and categorical) data, and build an xgboost classifier.\n\nJust because it's bugging me, here are a few things that may need to be improved in this model:\n\n1) There is probably a high correlation between the target variable and some feature variables. We can check this quickly. Fill the NAs with zero and correlate it with the loan status (which is 0 and 1).\n\n```{python}\nnumeric_fillna_df = numeric_df.fillna(0)\nnumeric_fillna_df.corrwith(y)\n```\nWe can see that *rate_of_interest* has a high inverse correlation with the target variable.  However, I did try removing *rate_of_interest* and still ended up with an F1 score of 0.9999.\n\n2) Find a better way to impute the missing categorical. Chirag Goyal enumerates some options in [this post](https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables/). I suspect that building a model to predict missing values would be an option.  Another simpler option would be to just randomly insert existing values.  But, currently, with the imputer in this post, it is essentially treating 'missing' as a legit value.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","theme":"journal","title-block-banner":true,"title":"Using xgboost and sklearn to Predict Loan Default","author":"Matt Pickard","date":"2022-03-01","categories":["python","machine learning"]},"extensions":{"book":{"multiFile":true}}}}}