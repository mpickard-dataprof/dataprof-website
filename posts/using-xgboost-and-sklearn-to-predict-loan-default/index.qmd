---
title: Using xgboost and sklearn to Predict Loan Default
author: Matt Pickard
date: '2022-03-01'
categories: [python, machine learning]
---

```{python}
import numpy as np 
import pandas as pd 
```

# Introduction
I wanted to combine `xgboost` with `sklearn` pipelines to process a `pandas` DataFrame. Special thanks to Kaggle user M Yasser H for supplying the [Loan Default Dataset](https://www.kaggle.com/yasserh/loan-default-dataset). 

# Load the data
```{python}
# Load the data
loan_df = pd.read_csv("Loan_Default.csv")
```

# Create labels and features
The loan status (whether or not the customer defaulted on the loan) is the target variable. We'll extract that out as our labels. The other columns (minus the *ID*) will serve as our features. And we'll take a peek at our features.
```{python}
# Split out labels and features, encode labels as integers
y = loan_df['Status']

X = loan_df.loc[:,~loan_df.columns.isin(['ID','Status'])]
```

# Simple data exploration
Take a peek at our feature variables.
```{python}
X.head()
```

Notice there is a mix of categorical and continuous variables.

Let's check if there are any missing values.
```{python}
X.isnull().sum()
```


# Preprocessing

Since we have a mix of continuous and categorical variables, we'll set up an imputer for each type of variable. So, we are going to separate the continuous and the categorical variables into separate DataFrames. 

For the continuous variables, we'll impute the median.
```{python}
from sklearn_pandas import DataFrameMapper
from sklearn.impute import SimpleImputer

# extract numeric columns
numeric_mask = (X.dtypes != object)
numeric_columns = X.columns[numeric_mask].tolist()
numeric_df = X[numeric_columns]

# create "imputer", just going to fill missing values with "missing"
numeric_imputor = DataFrameMapper(
  [([numeric_feature], SimpleImputer(strategy='median')) for numeric_feature in numeric_df],
  input_df=True,
  df_out=True
  )
```

For the categorical variables, we'll impute the value 'missing'.
```{python}
# extract categorical features
categorical_mask = (X.dtypes == object)
categorical_columns = X.columns[categorical_mask].tolist()
categorical_df = X[categorical_columns]

categorical_imputor = DataFrameMapper(
  [([categorical_feature], SimpleImputer(strategy='constant', fill_value = "missing")) for categorical_feature in categorical_df],
  input_df=True,
  df_out=True
  )
```

# Build the pipeline

We are going to use `sklearn`'s `DictVectorizer`, which operates on numpy arrays/matrices. So to make it compatible with DataFrames, we'll create a simple utility class to allow a DataFrame to be passed through the pipeline. Thanks to Chanseok for the [dictifier code](https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/07/03-Using-XGBoost-in-pipelines.html).
```{python}
from sklearn.base import BaseEstimator, TransformerMixin

# Define Dictifier class to turn df into a dictionary as part of the pipeline
class Dictifier(BaseEstimator, TransformerMixin):
  def fit(self, X, y=None):
    return self
  
  def transform(self, X):
    if type(X) == pd.core.frame.DataFrame:
      return X.to_dict("records")
    else:
      return pd.DataFrame(X).to_dict("records")
```

Now we build the pipeline.  Notice how we use the FeatureUnion to bring the continuous and categorical features back together again at the start of the pipeline.
```{python}
from sklearn.pipeline import FeatureUnion
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import DictVectorizer
import xgboost as xgb

imputed_df = FeatureUnion([
  ('num_imputer', numeric_imputor),
  ('cat_imputer', categorical_imputor)    
  ])
  
xgb_pipeline = Pipeline([
  ("featureunion", imputed_df),
  ('dictifier', Dictifier()),
  ('dict_vectorizer', DictVectorizer(sort=False)),
  ("xgb", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))
  ])
```

# 3-Fold Cross Validation
We'll use 3-fold cross-validation (instead of 10, or something greater) to minimize compute time.
```{python, warning=FALSE}
from sklearn.model_selection import cross_val_score

scores = cross_val_score(xgb_pipeline, X, y, scoring="f1", cv=3)
avg_f1 = np.mean(np.sqrt(np.abs(scores)))
print("Avg F1 Score:", avg_f1)
```

# Conclusions
The average F1 score is suspiciously high, so let's not put much clout on the quality of the model. But it serves the purpose of demonstrating how to pass a 'pandas' DataFrame through a `sklearn` pipeline, preprocess mixed variable (continuous and categorical) data, and build an xgboost classifier.

Just because it's bugging me, here are a few things that may need to be improved in this model:

1) There is probably a high correlation between the target variable and some feature variables. We can check this quickly. Fill the NAs with zero and correlate it with the loan status (which is 0 and 1).

```{python}
numeric_fillna_df = numeric_df.fillna(0)
numeric_fillna_df.corrwith(y)
```
We can see that *rate_of_interest* has a high inverse correlation with the target variable.  However, I did try removing *rate_of_interest* and still ended up with an F1 score of 0.9999.

2) Find a better way to impute the missing categorical. Chirag Goyal enumerates some options in [this post](https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables/). I suspect that building a model to predict missing values would be an option.  Another simpler option would be to just randomly insert existing values.  But, currently, with the imputer in this post, it is essentially treating 'missing' as a legit value.
