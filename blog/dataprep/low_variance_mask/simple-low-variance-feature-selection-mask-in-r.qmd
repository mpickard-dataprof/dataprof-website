---
title: Simple Feature Selection Using a Low-to-No Variance Mask
description: Demonstrates how to remove features with low to no variance.
author: Matt Pickard
date: 2023-01-05
date-modified: last-modified
categories: []
format: html
---

# Introduction
One way to perform dimensionality reduction is through feature selection. In this post, we'll explore how to create a low-variance feature mask (or filter). We'll cover both a manual approach with `dplyr` functions and a more automated, production approach with `recipes` functions.  Enjoy!

## Why is variance important in feature selection?

Variance is important in feature selection because of a concept called information gain. *Information gain* is what we know about one (usually unknown) variable because we can observe another known variable. In supervised learning, we use information from predictor variables to learn something about the target variable.

To make this concrete, imagine we want to estimate loan applicants' creditworthiness.  Though we don't have their credit scores, we do have their monthly income, age, and number of outstanding loans. If every applicant in our data set has three outstanding loans -- that is, the number of outstanding loans didn't vary -- then "outstanding loans" does not differentiate loan applicants and, therefore, does not provide any information about the applicants that we can use to estimate their creditworthiness. Therefore, we'd say that number of outstanding loans provides no information gain about creditworthiness.

We can remove features with little to no variance because they are not informative. Consider them useless fluff.

# A manual method 
We'll start by creating a low-variance filter manually with `dplyr` functions.

## Setup

For this example, we'll use a [credit score classification](https://www.kaggle.com/datasets/parisrohan/credit-score-classification) data set from Kaggle, provided by [Rohan Paris](https://www.kaggle.com/datasets/parisrohan/credit-score-classification). The data set is large, so I randomly sampled 20% of it. I also did a little data cleaning. As you'll see, to make this exercise interesting, we'll add a few low-variance features to demonstrate feature removal.

You can download the cleaned data set <a href="data/credit_data.csv" download>here</a>.

Since variance is conceptually simpler with continuous variables, let's only load the continuous variables into `credit_df`.

```{r}
library(tidyverse)
library(knitr)

credit_df <- 
  read_csv("data/credit_data.csv") %>% 
  select_if(is.numeric)
```

Here's a peek at the data.

```{r}
kable(credit_df %>% head(5))
```


## Calculate feature variances

To begin, let's calculate the variance of each column. With `dplyr`, we'll use `across()` to apply `var()` to all columns with the `everything()` selector. Notice that we `scale()` the data before passing it to `var()`. It is important to normalize the data so the features are comparable with each other. Unnormalized, `num_credit_card` and `monthly income`, have very different variances. To compare their influence on creditworthiness in a fair manner, we normalize them.

We use `tidyr`'s `pivot_longer()` to pivot the scaled variances to columns. We'll sort them from largest to smallest.
```{r}
credit_df %>% 
  summarize(
    across(everything(), ~ var(scale(., center = FALSE), na.rm = TRUE))) 
```

```{r}
credit_variances <- credit_df %>% 
  summarize(
    across(
        everything(), 
        ~ var(scale(., center = FALSE), na.rm = TRUE))) %>% 
  pivot_longer(
    everything(), 
    names_to = "feature", 
    values_to = "variance") %>% 
  arrange(desc(variance)) 

kable(credit_variances)
```

## Set variance threshold and create a mask

We can scan down the variances and identify a natural cut-off between `amount_invested_monthly` and `outstanding_debt`, however, that would remove too many features. The next cutoff between `credit_history_month` and `age` seems more appropriate. So, we can create a variance filter with a threshold of 0.1. 

We use `pull()` to get an array of feature names that we can use as a mask.

```{r}
low_var_filter <- credit_variances %>% 
  filter(variance < 0.1) %>% 
  pull(feature)

low_var_filter
```

## Apply the mask

We then apply the mask to the data frame. Notice that it removes 'age' and `credit_utilization_ratio`.

```{r}
filtered_credit_df <- credit_df %>% 
  select(-all_of(low_var_filter))

kable(filtered_credit_df %>% head(5))
```

# A better way -- `tidymodels`

Creating a low-variance mask manually is good for learning purposes, but in real-world scenarios, it's nice to have something more automated -- that will work in a production pipeline -- and more intelligent -- that can handle categorical variables. The `recipes` package in `tidymodels` provides `step_zv()` and `step_nzv()` to features with zero or near-zero variance, respectively.

## Load data with categorical variables

To demonstrate the `recipes` approach, we'll load all the features, so we have both continuous and categorical variables.

```{r}
credit_df <- 
  read_csv("data/credit_data.csv")
```

To make this example interesting, we'll insert a couple of low-variance features -- `num_credit_card` and `num_credit_inquiries`.

```{r}
credit_df <- credit_df %>% 
  mutate(
    num_credit_card_rand = runif(n()),
    num_credit_inquiries_rand = runif(n()),
    num_credit_card = if_else(
      num_credit_card_rand < .95, 5, num_credit_card),
    num_credit_inquiries = if_else(
      num_credit_inquiries_rand < .95, 3, num_credit_inquiries)
  ) %>% 
  select(-num_credit_card_rand, -num_credit_inquiries_rand)
```

## Define a recipe object

Then we define a recipe object. Notice the first parameter is a formula. We define `credit_score` as the target variable and all other features as predictor variables.

We add the `step_zv()` step first. No-variance features will cause problems when we normalize the data with `step_scale()`. We apply the no-variance step to all predictors and the scale step to only the numeric predictors. Then, we apply `step_nzv()` to remove low-variance features. `prep()` "fits" the recipe to the data.

```{r}
library(recipes) 

low_variance_recipe <- recipe(credit_score ~ ., data = credit_df) %>%
  step_zv(all_predictors()) %>%
  step_scale(all_numeric_predictors()) %>%
  step_nzv(all_predictors()) %>%
  prep()
```

We can use `tidy()` to peek into the recipe and see the effect it will have on the trained data set it was trained on. Here we look at the third step of the recipe -- `step_nzv()`. We can see that it will remove our two low-variance features -- `num_credit_card` and `num_credit_inquiries`.

```{r}
tidy(low_variance_recipe, number = 3)
```

## Apply the recipe to `credit_df`

In `recipes` to apply a trained recipe to a data set, we can use `bake()`. The `new_data` parameter allows us to specify the data set "bake" the recipe with. If we pass NULL, it will bake the same data that the recipe was trained on.

```{r}
filtered_credit_df <- low_variance_recipe %>% bake(new_data = NULL)

names(filtered_credit_df)
```

If we compare the names in the original `credit_df` to the names in the `filtered_credit_df`, we can see that `num_credit_card` and `num_credit_inquiries` were removed.

```{r}
setdiff(names(credit_df), names(filtered_credit_df))
```
