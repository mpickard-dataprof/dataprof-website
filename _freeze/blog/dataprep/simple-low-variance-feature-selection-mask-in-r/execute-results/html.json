{
  "hash": "498d245170351058cb4678d216198f36",
  "result": {
    "markdown": "---\ntitle: Simple Feature Selection Using a Low-to-No Variance Mask\ndescription: Demonstrates how to remove features with low to no variance.\nauthor: Matt Pickard\ndate: 2023-01-05\ndate-modified: last-modified\ncategories: []\nformat: html\n---\n\n\n# Introduction\nOne way to perform dimensionality reduction is through feature selection. In this post, we'll explore how to create a low-variance feature mask (or filter). We'll cover both a manual approach with `dplyr` functions and a more automated, production approach with `recipes` functions.  Enjoy!\n\n## Why is variance important in feature selection?\n\nVariance is important in feature selection because of a concept called information gain. *Information gain* is what we know about one (usually unknown) variable because we can observe another known variable. In supervised learning, we use information from predictor variables to learn something about the target variable.\n\nTo make this concrete, imagine we want to estimate loan applicants' creditworthiness.  Though we don't have their credit scores, we do have their monthly income, age, and number of outstanding loans. If every applicant in our data set has three outstanding loans -- that is, the number of outstanding loans didn't vary -- then \"outstanding loans\" does not differentiate loan applicants and, therefore, does not provide any information about the applicants that we can use to estimate their creditworthiness. Therefore, we'd say that number of outstanding loans provides no information gain about creditworthiness.\n\nWe can remove features with little to no variance because they are not informative. Consider them useless fluff.\n\n# A manual method \nWe'll start by creating a low-variance filter manually with `dplyr` functions.\n\n## Setup\n\nFor this example, we'll use a [credit score classification](https://www.kaggle.com/datasets/parisrohan/credit-score-classification) data set from Kaggle, provided by [Rohan Paris](https://www.kaggle.com/datasets/parisrohan/credit-score-classification). The data set is large, so I randomly sampled 20% of it. I also did a little data cleaning. As you'll see, to make this exercise interesting, we'll add a few low-variance features to demonstrate feature removal.\n\nYou can download the cleaned data set <a href=\"data/credit_data.csv\" download>here</a>.\n\nSince variance is conceptually simpler with continuous variables, let's only load the continuous variables into `credit_df`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(knitr)\n\ncredit_df <- \n  read_csv(\"data/credit_data.csv\") %>% \n  select_if(is.numeric)\n```\n:::\n\n\nHere's a peek at the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(credit_df %>% head(5))\n```\n\n::: {.cell-output-display}\n| age| annual_income| monthly_inhand_salary| num_bank_accounts| num_credit_card| interest_rate| num_of_loan| delay_from_due_date| num_of_delayed_payment| changed_credit_limit| num_credit_inquiries| outstanding_debt| credit_utilization_ratio| total_emi_per_month| amount_invested_monthly| monthly_balance| credit_history_months|\n|---:|-------------:|---------------------:|-----------------:|---------------:|-------------:|-----------:|-------------------:|----------------------:|--------------------:|--------------------:|----------------:|------------------------:|-------------------:|-----------------------:|---------------:|---------------------:|\n|  41|      16176.83|              1585.070|                 8|               3|            10|           3|                  17|                     14|                 0.56|                 2445|          1200.70|                 29.94733|            21.96219|                38.32601|        358.2188|                   241|\n|  36|      82383.04|              6661.253|                 3|               5|             8|           4|                  17|                     14|                 1.71|                    2|          1218.57|                 33.38963|           162.10896|               123.97997|        630.0364|                   392|\n|  44|      28805.34|              2309.445|                 8|               3|             5|           2|                  13|                     19|                 7.02|                    0|           796.45|                 26.83209|            47.19512|               139.90769|        303.8417|                   385|\n|  28|      45412.95|              3520.412|                 8|               6|            30|           6|                  28|                     21|                23.07|                    6|          4601.39|                 23.41958|           113.69155|               199.56341|        318.7863|                    55|\n|  45|      17296.38|              1480.365|                 6|              10|            30|           5|                  21|                     17|                17.34|                    7|          4624.73|                 38.39057|            49.36071|                78.16123|        280.5146|                    92|\n:::\n:::\n\n\n\n## Calculate feature variances\n\nTo begin, let's calculate the variance of each column. With `dplyr`, we'll use `across()` to apply `var()` to all columns with the `everything()` selector. Notice that we `scale()` the data before passing it to `var()`. It is important to normalize the data so the features are comparable with each other. Unnormalized, `num_credit_card` and `monthly income`, have very different variances. To compare their influence on creditworthiness in a fair manner, we normalize them.\n\nWe use `tidyr`'s `pivot_longer()` to pivot the scaled variances to columns. We'll sort them from largest to smallest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncredit_variances <- credit_df %>% \n  summarize(\n    across(\n        everything(), \n        ~ var(scale(., center = FALSE)), \n        na.rm = TRUE)) %>% \n  pivot_longer(\n    everything(), \n    names_to = \"feature\", \n    values_to = \"variance\") %>% \n  arrange(desc(variance)) \n\nkable(credit_variances)\n```\n\n::: {.cell-output-display}\n|feature                  |   variance|\n|:------------------------|----------:|\n|num_of_loan              | 0.98558800|\n|interest_rate            | 0.98258481|\n|num_of_delayed_payment   | 0.98150878|\n|num_credit_inquiries     | 0.97843457|\n|num_bank_accounts        | 0.97841843|\n|total_emi_per_month      | 0.97569764|\n|annual_income            | 0.97397017|\n|num_credit_card          | 0.97044729|\n|amount_invested_monthly  | 0.90345646|\n|outstanding_debt         | 0.41753915|\n|monthly_inhand_salary    | 0.35255863|\n|delay_from_due_date      | 0.33223435|\n|changed_credit_limit     | 0.30855481|\n|monthly_balance          | 0.22081631|\n|credit_history_months    | 0.15974632|\n|age                      | 0.07874258|\n|credit_utilization_ratio | 0.02527138|\n:::\n:::\n\n\n## Set variance threshold and create a mask\n\nWe can scan down the variances and identify a natural cut-off between `amount_invested_monthly` and `outstanding_debt`, however, that would remove too many features. The next cutoff between `credit_history_month` and `age` seems more appropriate. So, we can create a variance filter with a threshold of 0.1. \n\nWe use `pull()` to get an array of feature names that we can use as a mask.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlow_var_filter <- credit_variances %>% \n  filter(variance < 0.1) %>% \n  pull(feature)\n\nlow_var_filter\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"age\"                      \"credit_utilization_ratio\"\n```\n:::\n:::\n\n\n## Apply the mask\n\nWe then apply the mask to the data frame. Notice that it removes 'age' and `credit_utilization_ratio`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltered_credit_df <- credit_df %>% \n  select(-all_of(low_var_filter))\n\nkable(filtered_credit_df %>% head(5))\n```\n\n::: {.cell-output-display}\n| annual_income| monthly_inhand_salary| num_bank_accounts| num_credit_card| interest_rate| num_of_loan| delay_from_due_date| num_of_delayed_payment| changed_credit_limit| num_credit_inquiries| outstanding_debt| total_emi_per_month| amount_invested_monthly| monthly_balance| credit_history_months|\n|-------------:|---------------------:|-----------------:|---------------:|-------------:|-----------:|-------------------:|----------------------:|--------------------:|--------------------:|----------------:|-------------------:|-----------------------:|---------------:|---------------------:|\n|      16176.83|              1585.070|                 8|               3|            10|           3|                  17|                     14|                 0.56|                 2445|          1200.70|            21.96219|                38.32601|        358.2188|                   241|\n|      82383.04|              6661.253|                 3|               5|             8|           4|                  17|                     14|                 1.71|                    2|          1218.57|           162.10896|               123.97997|        630.0364|                   392|\n|      28805.34|              2309.445|                 8|               3|             5|           2|                  13|                     19|                 7.02|                    0|           796.45|            47.19512|               139.90769|        303.8417|                   385|\n|      45412.95|              3520.412|                 8|               6|            30|           6|                  28|                     21|                23.07|                    6|          4601.39|           113.69155|               199.56341|        318.7863|                    55|\n|      17296.38|              1480.365|                 6|              10|            30|           5|                  21|                     17|                17.34|                    7|          4624.73|            49.36071|                78.16123|        280.5146|                    92|\n:::\n:::\n\n\n# A better way -- `tidymodels`\n\nCreating a low-variance mask manually is good for learning purposes, but in real-world scenarios, it's nice to have something more automated -- that will work in a production pipeline -- and more intelligent -- that can handle categorical variables. The `recipes` package in `tidymodels` provides `step_zv()` and `step_nzv()` to features with zero or near-zero variance, respectively.\n\n## Load data with categorical variables\n\nTo demonstrate the `recipes` approach, we'll load all the features, so we have both continuous and categorical variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncredit_df <- \n  read_csv(\"data/credit_data.csv\")\n```\n:::\n\n\nTo make this example interesting, we'll insert a couple of low-variance features -- `num_credit_card` and `num_credit_inquiries`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncredit_df <- credit_df %>% \n  mutate(\n    num_credit_card_rand = runif(n()),\n    num_credit_inquiries_rand = runif(n()),\n    num_credit_card = if_else(\n      num_credit_card_rand < .95, 5, num_credit_card),\n    num_credit_inquiries = if_else(\n      num_credit_inquiries_rand < .95, 3, num_credit_inquiries)\n  ) %>% \n  select(-num_credit_card_rand, -num_credit_inquiries_rand)\n```\n:::\n\n\n## Define a recipe object\n\nThen we define a recipe object. Notice the first parameter is a formula. We define `credit_score` as the target variable and all other features as predictor variables.\n\nWe add the `step_zv()` step first. No-variance features will cause problems when we normalize the data with `step_scale()`. We apply the no-variance step to all predictors and the scale step to only the numeric predictors. Then, we apply `step_nzv()` to remove low-variance features. `prep()` \"fits\" the recipe to the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes) \n\nlow_variance_recipe <- recipe(credit_score ~ ., data = credit_df) %>%\n  step_zv(all_predictors()) %>%\n  step_scale(all_numeric_predictors()) %>%\n  step_nzv(all_predictors()) %>%\n  prep()\n```\n:::\n\n\nWe can use `tidy()` to peek into the recipe and see the effect it will have on the trained data set it was trained on. Here we look at the third step of the recipe -- `step_nzv()`. We can see that it will remove our two low-variance features -- `num_credit_card` and `num_credit_inquiries`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(low_variance_recipe, number = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  terms                id       \n  <chr>                <chr>    \n1 num_credit_card      nzv_5xxFQ\n2 num_credit_inquiries nzv_5xxFQ\n```\n:::\n:::\n\n\n## Apply the recipe to `credit_df`\n\nIn `recipes` to apply a trained recipe to a data set, we can use `bake()`. The `new_data` parameter allows us to specify the data set \"bake\" the recipe with. If we pass NULL, it will bake the same data that the recipe was trained on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltered_credit_df <- low_variance_recipe %>% bake(new_data = NULL)\n\nnames(filtered_credit_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"month\"                    \"age\"                     \n [3] \"occupation\"               \"annual_income\"           \n [5] \"monthly_inhand_salary\"    \"num_bank_accounts\"       \n [7] \"interest_rate\"            \"num_of_loan\"             \n [9] \"delay_from_due_date\"      \"num_of_delayed_payment\"  \n[11] \"changed_credit_limit\"     \"outstanding_debt\"        \n[13] \"credit_utilization_ratio\" \"payment_of_min_amount\"   \n[15] \"total_emi_per_month\"      \"amount_invested_monthly\" \n[17] \"payment_behaviour\"        \"monthly_balance\"         \n[19] \"credit_history_months\"    \"credit_score\"            \n```\n:::\n:::\n\n\nIf we compare the names in the original `credit_df` to the names in the `filtered_credit_df`, we can see that `num_credit_card` and `num_credit_inquiries` were removed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetdiff(names(credit_df), names(filtered_credit_df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"num_credit_card\"      \"num_credit_inquiries\"\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}