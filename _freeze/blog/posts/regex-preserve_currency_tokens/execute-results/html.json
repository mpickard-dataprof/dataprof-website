{
  "hash": "3e0c7fffd4f427a121269639a8097bab",
  "result": {
    "markdown": "---\ntitle: regex to Preserve Currency Tokens\ndescription: Development of a regex to preserve currency values in the U.S. Tax Code.\nauthor: Matt Pickard\ndate: '2022-03-14'\ncategories: [regex, data preparation]\n---\n\n\n# Introduction\nI've been working to collect a corpus of tax legislation, regulations, and court cases. At the core of my corpus is the United States Code Title 26 (USC26), otherwise known as the Internal Revenue Code or the U.S. Tax Code. I'm using the corpus to train a tax-specific word embedding. Word embeddings capture similarity semantics of tokens (words, or multi-word phrases) by encoding them into a n-dimensional space. Similar tokens are embedded into the n-dimensional space in close proximity.\n\nSince currency values may have important semantic meaning in the USC--for instance, tax bracket boundaries--I want to preserve them so the tokenizer does not break them apart. To do this, I want to replace spaces, commas, decimals, and dollar signs with underscores.  Here are some examples:\n\n| Original     | Preserved       |\n|--------------|-----------------|\n| $172,175     | 172_175_usd     |\n| $2.5 million | 2_5_million_usd |\n| $56,750.25   | 56_750_25_usd   |\n\n\n# Implementation\nThe approach I took was to first match the different currency formats and then pass the match to a helper function that would replace spaces, commas, decimals, and dollar signs with underscores.\n\nHere is the pattern I created to match currency tokens:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n```\n:::\n\n\n* `\\$` matches a dollar sign. \n* `\\d{0,3}` The number after the dollar sign and before a comma or decimal boundary. Looking for 0 to 3 digits allows for these patterns: \\$.75, \\$125,000, etc.\n* `(\\,\\d{3}){0,4}(\\.\\d{1,2})?` matches repeated comma boundaries (if they exist) and two-decimal places for cents (if they exist). More specifically:\n  * `(\\,\\d{3})` matches \",xxx\". With the addition of `{0,4}` it matches \",xxx\" repeatedly--up to trillions (which is probably overkill for the USC26 because most large currency values have \"million\" or \"billion\" text suffixes).\n  * `(\\.\\d{1,2})?` matches an optional cents.\n* `( (million|billion))?` just checks for a text suffix (the `?` at the end makes it optional.)\n\nNext, here is a simple helper function to replace spaces, commas, decimals, and dollar signs with underscores and tack on \"_usd\" at the end. Python's string `replace()` method is faster than the regex `sub()`, so I went with `replace()`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport re\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n```\n:::\n\n\nNow I need a test string.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntest_string = \"I made $755.34 billion this year, $5.34 million last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45 in case that's a more realistic salary in my life.\"\n```\n:::\n\n\nThen I compile the regex and replace matches.  Notice that the second argument of `sub()` can take a method that returns a string.  I leverage this to call the helper function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\"I made 755_34_billion_usd this year, 5_34_million_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd in case that's a more realistic salary in my life.\"\n```\n:::\n:::\n\n\nHere is the full Python implementation.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\nimport timeit\n\npattern = r'\\$\\d{0,3}(\\,\\d{3}){0,4}(\\.\\d{1,2})?( (million|billion))?'\n\ndef replace_currency_parts(match):\n    text = match.group(0).replace(\" \", \"_\").replace(\".\", \"_\").replace(',', '_')\n    if (text[0] == '$'):\n        text = text[1:] + \"_usd\"\n    return text\n\ntest_string = \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\ncompiled_pattern = re.compile(pattern)\ncompiled_pattern.sub(replace_currency_parts, test_string)\n```\n:::\n\n\n\n\n# R Implementation\nI was curious if python was that much faster than R.  So, here is an R implementation. Like Python's `sub()`, the `str_replace_all` function can take a helper function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stringr)\n\npattern <- '\\\\$\\\\d{0,3}(\\\\,\\\\d{3}){0,4}(\\\\.\\\\d{1,2})?( (million|billion))?'\n\nreplace_currency_parts <- function(match) {\n  \n  text <- match %>% \n    str_replace_all(\" \", \"_\") %>% \n    str_replace_all(\"\\\\.\", \"_\") %>% \n    str_replace_all(\"\\\\,\", \"_\")\n    \n  if (str_sub(text, 1, 1) == \"$\"){\n    text <- str_c(str_sub(text, 2), \"_usd\")\n  }\n    \n  return(text)\n}\n\ntest_string <-  \"I made $355.34 million this year, $435.34 billion last year, but I also want to match $125,234.34 and $1,342.40 and $45.09 and $45.\"\n\nstr_replace_all(test_string, pattern, replace_currency_parts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"I made 355_34_million_usd this year, 435_34_billion_usd last year, but I also want to match 125_234_34_usd and 1_342_40_usd and 45_09_usd and 45_usd.\"\n```\n:::\n:::\n\n\n# Which is faster?\nSo, which is faster?\n\nI compare the Python and R implementations by taking the average of 1000 executions of the code.  Since the {stringr} package in R is higher level--there's no option to compile the regex ahead of time and keep it out of the loop--I placed the Python regex compile code inside the loop to make a more fair comparison.  In both cases, the units are milliseconds. NOTE: The python `time()` method returns seconds, so I convert.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom time import time\n\nstart_ms = int(round(time() * 1000))\nfor i in range(1000):\n  compiled_pattern = re.compile(pattern)\n  str = compiled_pattern.sub(replace_currency_parts, test_string)\nend_ms = int(round(time() * 1000))\n\nprint(\"elapsed ms = \", (end_ms-start_ms)/1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nelapsed ms =  0.035\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(microbenchmark)\n\nmicrobenchmark::microbenchmark(str_replace_all(test_string, pattern, replace_currency_parts), times = 1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                                                          expr    min     lq\n str_replace_all(test_string, pattern, replace_currency_parts) 1.9369 2.2627\n     mean  median      uq     max neval\n 2.945467 2.43615 3.05885 18.2587  1000\n```\n:::\n:::\n\n\nI don't know what overhead is included in the {stringr} package, but it's about two orders of magnitude slower than Python. I suspect it is because the Python `replace()` string method is quite speedy.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}