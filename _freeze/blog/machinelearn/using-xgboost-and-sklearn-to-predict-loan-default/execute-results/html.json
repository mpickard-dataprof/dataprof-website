{
  "hash": "14a81a7006dc6177f43df96a9711441b",
  "result": {
    "markdown": "---\ntitle: Using xgboost and sklearn to Predict Loan Default\nauthor: Matt Pickard\ndate: '2022-03-01'\ncategories: [python, machine learning]\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np \nimport pandas as pd \n```\n:::\n\n\n# Introduction\nI wanted to combine `xgboost` with `sklearn` pipelines to process a `pandas` DataFrame. Special thanks to Kaggle user M Yasser H for supplying the [Loan Default Dataset](https://www.kaggle.com/yasserh/loan-default-dataset). \n\n# Load the data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Load the data\nloan_df = pd.read_csv(\"data/Loan_Default.csv\")\n```\n:::\n\n\n# Create labels and features\nThe loan status (whether or not the customer defaulted on the loan) is the target variable. We'll extract that out as our labels. The other columns (minus the *ID*) will serve as our features. And we'll take a peek at our features.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Split out labels and features, encode labels as integers\ny = loan_df['Status']\n\nX = loan_df.loc[:,~loan_df.columns.isin(['ID','Status'])]\n```\n:::\n\n\n# Simple data exploration\nTake a peek at our feature variables.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nX.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>loan_limit</th>\n      <th>Gender</th>\n      <th>approv_in_adv</th>\n      <th>loan_type</th>\n      <th>loan_purpose</th>\n      <th>Credit_Worthiness</th>\n      <th>open_credit</th>\n      <th>business_or_commercial</th>\n      <th>loan_amount</th>\n      <th>...</th>\n      <th>income</th>\n      <th>credit_type</th>\n      <th>Credit_Score</th>\n      <th>co-applicant_credit_type</th>\n      <th>age</th>\n      <th>submission_of_application</th>\n      <th>LTV</th>\n      <th>Region</th>\n      <th>Security_Type</th>\n      <th>dtir1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019</td>\n      <td>cf</td>\n      <td>Sex Not Available</td>\n      <td>nopre</td>\n      <td>type1</td>\n      <td>p1</td>\n      <td>l1</td>\n      <td>nopc</td>\n      <td>nob/c</td>\n      <td>116500</td>\n      <td>...</td>\n      <td>1740.0</td>\n      <td>EXP</td>\n      <td>758</td>\n      <td>CIB</td>\n      <td>25-34</td>\n      <td>to_inst</td>\n      <td>98.728814</td>\n      <td>south</td>\n      <td>direct</td>\n      <td>45.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019</td>\n      <td>cf</td>\n      <td>Male</td>\n      <td>nopre</td>\n      <td>type2</td>\n      <td>p1</td>\n      <td>l1</td>\n      <td>nopc</td>\n      <td>b/c</td>\n      <td>206500</td>\n      <td>...</td>\n      <td>4980.0</td>\n      <td>EQUI</td>\n      <td>552</td>\n      <td>EXP</td>\n      <td>55-64</td>\n      <td>to_inst</td>\n      <td>NaN</td>\n      <td>North</td>\n      <td>direct</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019</td>\n      <td>cf</td>\n      <td>Male</td>\n      <td>pre</td>\n      <td>type1</td>\n      <td>p1</td>\n      <td>l1</td>\n      <td>nopc</td>\n      <td>nob/c</td>\n      <td>406500</td>\n      <td>...</td>\n      <td>9480.0</td>\n      <td>EXP</td>\n      <td>834</td>\n      <td>CIB</td>\n      <td>35-44</td>\n      <td>to_inst</td>\n      <td>80.019685</td>\n      <td>south</td>\n      <td>direct</td>\n      <td>46.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019</td>\n      <td>cf</td>\n      <td>Male</td>\n      <td>nopre</td>\n      <td>type1</td>\n      <td>p4</td>\n      <td>l1</td>\n      <td>nopc</td>\n      <td>nob/c</td>\n      <td>456500</td>\n      <td>...</td>\n      <td>11880.0</td>\n      <td>EXP</td>\n      <td>587</td>\n      <td>CIB</td>\n      <td>45-54</td>\n      <td>not_inst</td>\n      <td>69.376900</td>\n      <td>North</td>\n      <td>direct</td>\n      <td>42.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019</td>\n      <td>cf</td>\n      <td>Joint</td>\n      <td>pre</td>\n      <td>type1</td>\n      <td>p1</td>\n      <td>l1</td>\n      <td>nopc</td>\n      <td>nob/c</td>\n      <td>696500</td>\n      <td>...</td>\n      <td>10440.0</td>\n      <td>CRIF</td>\n      <td>602</td>\n      <td>EXP</td>\n      <td>25-34</td>\n      <td>not_inst</td>\n      <td>91.886544</td>\n      <td>North</td>\n      <td>direct</td>\n      <td>39.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 32 columns</p>\n</div>\n```\n:::\n:::\n\n\nNotice there is a mix of categorical and continuous variables.\n\nLet's check if there are any missing values.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nX.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nyear                             0\nloan_limit                    3344\nGender                           0\napprov_in_adv                  908\nloan_type                        0\nloan_purpose                   134\nCredit_Worthiness                0\nopen_credit                      0\nbusiness_or_commercial           0\nloan_amount                      0\nrate_of_interest             36439\nInterest_rate_spread         36639\nUpfront_charges              39642\nterm                            41\nNeg_ammortization              121\ninterest_only                    0\nlump_sum_payment                 0\nproperty_value               15098\nconstruction_type                0\noccupancy_type                   0\nSecured_by                       0\ntotal_units                      0\nincome                        9150\ncredit_type                      0\nCredit_Score                     0\nco-applicant_credit_type         0\nage                            200\nsubmission_of_application      200\nLTV                          15098\nRegion                           0\nSecurity_Type                    0\ndtir1                        24121\ndtype: int64\n```\n:::\n:::\n\n\n# Preprocessing\n\nSince we have a mix of continuous and categorical variables, we'll set up an imputer for each type of variable. So, we are going to separate the continuous and the categorical variables into separate DataFrames. \n\nFor the continuous variables, we'll impute the median.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\n\n# extract numeric columns\nnumeric_mask = (X.dtypes != object)\nnumeric_columns = X.columns[numeric_mask].tolist()\nnumeric_df = X[numeric_columns]\n\n# create \"imputer\", just going to fill missing values with \"missing\"\nnumeric_imputor = DataFrameMapper(\n  [([numeric_feature], SimpleImputer(strategy='median')) for numeric_feature in numeric_df],\n  input_df=True,\n  df_out=True\n  )\n```\n:::\n\n\nFor the categorical variables, we'll impute the value 'missing'.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# extract categorical features\ncategorical_mask = (X.dtypes == object)\ncategorical_columns = X.columns[categorical_mask].tolist()\ncategorical_df = X[categorical_columns]\n\ncategorical_imputor = DataFrameMapper(\n  [([categorical_feature], SimpleImputer(strategy='constant', fill_value = \"missing\")) for categorical_feature in categorical_df],\n  input_df=True,\n  df_out=True\n  )\n```\n:::\n\n\n# Build the pipeline\n\nWe are going to use `sklearn`'s `DictVectorizer`, which operates on numpy arrays/matrices. So to make it compatible with DataFrames, we'll create a simple utility class to allow a DataFrame to be passed through the pipeline. Thanks to Chanseok for the [dictifier code](https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/07/03-Using-XGBoost-in-pipelines.html).\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define Dictifier class to turn df into a dictionary as part of the pipeline\nclass Dictifier(BaseEstimator, TransformerMixin):\n  def fit(self, X, y=None):\n    return self\n  \n  def transform(self, X):\n    if type(X) == pd.core.frame.DataFrame:\n      return X.to_dict(\"records\")\n    else:\n      return pd.DataFrame(X).to_dict(\"records\")\n```\n:::\n\n\nNow we build the pipeline.  Notice how we use the FeatureUnion to bring the continuous and categorical features back together again at the start of the pipeline.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nimport xgboost as xgb\n\nimputed_df = FeatureUnion([\n  ('num_imputer', numeric_imputor),\n  ('cat_imputer', categorical_imputor)    \n  ])\n  \nxgb_pipeline = Pipeline([\n  (\"featureunion\", imputed_df),\n  ('dictifier', Dictifier()),\n  ('dict_vectorizer', DictVectorizer(sort=False)),\n  (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n  ])\n```\n:::\n\n\n# 3-Fold Cross Validation\nWe'll use 3-fold cross-validation (instead of 10, or something greater) to minimize compute time.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb_pipeline, X, y, scoring=\"f1\", cv=3)\navg_f1 = np.mean(np.sqrt(np.abs(scores)))\nprint(\"Avg F1 Score:\", avg_f1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAvg F1 Score: 0.9999863537583441\n```\n:::\n:::\n\n\n# Conclusions\nThe average F1 score is suspiciously high, so let's not put much clout on the quality of the model. But it serves the purpose of demonstrating how to pass a 'pandas' DataFrame through a `sklearn` pipeline, preprocess mixed variable (continuous and categorical) data, and build an xgboost classifier.\n\nJust because it's bugging me, here are a few things that may need to be improved in this model:\n\n1) There is probably a high correlation between the target variable and some feature variables. We can check this quickly. Fill the NAs with zero and correlate it with the loan status (which is 0 and 1).\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nnumeric_fillna_df = numeric_df.fillna(0)\nnumeric_fillna_df.corrwith(y)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nyear                         NaN\nloan_amount            -0.036825\nrate_of_interest       -0.958875\nInterest_rate_spread   -0.392977\nUpfront_charges        -0.431183\nterm                   -0.000675\nproperty_value         -0.273267\nincome                 -0.044620\nCredit_Score            0.004004\nLTV                    -0.267700\ndtir1                  -0.325613\ndtype: float64\n```\n:::\n:::\n\n\nWe can see that *rate_of_interest* has a high inverse correlation with the target variable.  However, I did try removing *rate_of_interest* and still ended up with an F1 score of 0.9999.\n\n2) Find a better way to impute the missing categorical. Chirag Goyal enumerates some options in [this post](https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables/). I suspect that building a model to predict missing values would be an option.  Another simpler option would be to just randomly insert existing values.  But, currently, with the imputer in this post, it is essentially treating 'missing' as a legit value.\n\n",
    "supporting": [
      "using-xgboost-and-sklearn-to-predict-loan-default_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}